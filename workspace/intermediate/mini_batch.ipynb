{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_classification, make_regression\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチ学習\n",
    "---\n",
    "確率的勾配降下法のように、データセットのミニバッチを使って少しずつ学習させていく手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonでのミニバッチ作成\n",
    "---\n",
    "`yield`を使用してジェネレーターを作成する。  \n",
    "DB からの読み込みと組み合わせると、サイズの大きいデータセットでも学習可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, x, y=None, batch_size=10, random_state=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.n_data = len(x)\n",
    "        self.step = 0\n",
    "        self.step_per_epoch = ceil(self.n_data / self.batch_size)\n",
    "        self.index = np.arange(self.n_data)\n",
    "        if random_state:\n",
    "            random.seed(random_state)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.step == 0:\n",
    "            random.shuffle(self.index)\n",
    "        begin = self.batch_size * self.step\n",
    "        end = min(begin + self.batch_size, self.n_data)\n",
    "        batch_x = self.x[begin:end]\n",
    "        if self.y is None:\n",
    "            batch = batch_x\n",
    "        else:\n",
    "            batch_y = self.y[begin:end]\n",
    "            batch = (batch_x, batch_y)\n",
    "        self.step = (self.step + 1) % self.step_per_epoch\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85958841 -0.6365235 ]\n",
      " [-0.33407737  0.00211836]\n",
      " [-0.20393287 -0.18217541]] [-9.7636448  -3.49365597 -7.584586  ]\n",
      "[[ 0.01569637 -2.24268495]\n",
      " [-0.8596683   0.22598549]\n",
      " [ 1.03380073 -2.40045363]] [-66.47629433  -2.43671178 -60.32574025]\n"
     ]
    }
   ],
   "source": [
    "x, y = make_regression(n_samples=100, n_features=2, random_state=1234)\n",
    "gen = BatchGenerator(x, y, batch_size=3, random_state=1234)\n",
    "for _ in range(2):\n",
    "    x, y = next(gen)\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learnのミニバッチ学習実装\n",
    "---\n",
    "`partial_fit`メソッドにミニバッチを与えて学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch K-Means\n",
    "---\n",
    "ミニバッチ学習による K-Means クラスタリング。  \n",
    "`sklearn.cluster.MiniBatchKMeans`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MiniBatchKMeans in module sklearn.cluster._kmeans:\n",
      "\n",
      "class MiniBatchKMeans(KMeans)\n",
      " |  MiniBatchKMeans(n_clusters=8, *, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      " |  \n",
      " |  Mini-Batch K-Means clustering.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, default=8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  init : {'k-means++', 'random'} or ndarray of shape             (n_clusters, n_features), default='k-means++'\n",
      " |      Method for initialization\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose k observations (rows) at random from data for\n",
      " |      the initial centroids.\n",
      " |  \n",
      " |      If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations over the complete dataset before\n",
      " |      stopping independently of any early stopping criterion heuristics.\n",
      " |  \n",
      " |  batch_size : int, default=100\n",
      " |      Size of the mini batches.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  compute_labels : bool, default=True\n",
      " |      Compute label assignment and inertia for the complete dataset\n",
      " |      once the minibatch optimization has converged in fit.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for centroid initialization and\n",
      " |      random reassignment. Use an int to make the randomness deterministic.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=0.0\n",
      " |      Control early stopping based on the relative center changes as\n",
      " |      measured by a smoothed, variance-normalized of the mean center\n",
      " |      squared position changes. This early stopping heuristics is\n",
      " |      closer to the one used for the batch variant of the algorithms\n",
      " |      but induces a slight computational and memory overhead over the\n",
      " |      inertia heuristic.\n",
      " |  \n",
      " |      To disable convergence detection based on normalized center\n",
      " |      change, set tol to 0.0 (default).\n",
      " |  \n",
      " |  max_no_improvement : int, default=10\n",
      " |      Control early stopping based on the consecutive number of mini\n",
      " |      batches that does not yield an improvement on the smoothed inertia.\n",
      " |  \n",
      " |      To disable convergence detection based on inertia, set\n",
      " |      max_no_improvement to None.\n",
      " |  \n",
      " |  init_size : int, default=None\n",
      " |      Number of samples to randomly sample for speeding up the\n",
      " |      initialization (sometimes at the expense of accuracy): the\n",
      " |      only algorithm is initialized by running a batch KMeans on a\n",
      " |      random subset of the data. This needs to be larger than n_clusters.\n",
      " |  \n",
      " |      If `None`, `init_size= 3 * batch_size`.\n",
      " |  \n",
      " |  n_init : int, default=3\n",
      " |      Number of random initializations that are tried.\n",
      " |      In contrast to KMeans, the algorithm is only run once, using the\n",
      " |      best of the ``n_init`` initializations as measured by inertia.\n",
      " |  \n",
      " |  reassignment_ratio : float, default=0.01\n",
      " |      Control the fraction of the maximum number of counts for a\n",
      " |      center to be reassigned. A higher value means that low count\n",
      " |      centers are more easily reassigned, which means that the\n",
      " |      model will take longer to converge, but should converge in a\n",
      " |      better clustering.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      " |      Coordinates of cluster centers\n",
      " |  \n",
      " |  labels_ : int\n",
      " |      Labels of each point (if compute_labels is set to True).\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      The value of the inertia criterion associated with the chosen\n",
      " |      partition (if compute_labels is set to True). The inertia is\n",
      " |      defined as the sum of square distances of samples to their nearest\n",
      " |      neighbor.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  KMeans\n",
      " |      The classic implementation of the clustering method based on the\n",
      " |      Lloyd's algorithm. It consumes the whole set of input data at each\n",
      " |      iteration.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.cluster import MiniBatchKMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [4, 2], [4, 0], [4, 4],\n",
      " |  ...               [4, 5], [0, 1], [2, 2],\n",
      " |  ...               [3, 2], [5, 5], [1, -1]])\n",
      " |  >>> # manually fit on batches\n",
      " |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      " |  ...                          random_state=0,\n",
      " |  ...                          batch_size=6)\n",
      " |  >>> kmeans = kmeans.partial_fit(X[0:6,:])\n",
      " |  >>> kmeans = kmeans.partial_fit(X[6:12,:])\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[2. , 1. ],\n",
      " |         [3.5, 4.5]])\n",
      " |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      " |  array([0, 1], dtype=int32)\n",
      " |  >>> # fit on the whole data\n",
      " |  >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
      " |  ...                          random_state=0,\n",
      " |  ...                          batch_size=6,\n",
      " |  ...                          max_iter=10).fit(X)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[3.95918367, 2.40816327],\n",
      " |         [1.12195122, 1.3902439 ]])\n",
      " |  >>> kmeans.predict([[0, 0], [4, 4]])\n",
      " |  array([1, 0], dtype=int32)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MiniBatchKMeans\n",
      " |      KMeans\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, *, init='k-means++', max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute the centroids on X by chunking it into mini-batches.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
      " |          Training instances to cluster. It must be noted that the data\n",
      " |          will be converted to C ordering, which will cause a memory copy\n",
      " |          if the given data is not C-contiguous.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |          .. versionadded:: 0.20\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  partial_fit(self, X, y=None, sample_weight=None)\n",
      " |      Update k means estimate on a single mini-batch X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Coordinates of the data points to cluster. It must be noted that\n",
      " |          X will be copied if it is not C-contiguous.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  predict(self, X, sample_weight=None)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), optional\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight (default: None).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : array, shape [n_samples,]\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from KMeans:\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, sample_weight=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : ndarray of shape (n_samples,)\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, sample_weight=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers.  Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MiniBatchKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cluster, _ = make_blobs(n_samples=100000, n_features=2, random_state=1234)\n",
    "gen_cluster = BatchGenerator(x_cluster, batch_size=1000, random_state=1234)\n",
    "km = MiniBatchKMeans(n_clusters=3, random_state=1234)\n",
    "tol = 100\n",
    "best_score = np.inf\n",
    "early_stopping = 10\n",
    "no_improvement = 0\n",
    "while(best_score > tol):\n",
    "    batch = next(gen_cluster)\n",
    "    km.partial_fit(batch)\n",
    "    if best_score <= km.inertia_:\n",
    "        no_improvement += 1\n",
    "    else:\n",
    "        no_improvement = 0\n",
    "        best_score = km.inertia_\n",
    "    if no_improvement == early_stopping:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier\n",
    "---\n",
    "確率的勾配降下法 (SGD) による分類。`loss='hinge'`で SVM ・`loss='log'`でロジスティック回帰の SGD 版になる。  \n",
    "`sklearn.linear_model.SGDClassifier`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGDClassifier in module sklearn.linear_model._stochastic_gradient:\n",
      "\n",
      "class SGDClassifier(BaseSGDClassifier)\n",
      " |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      " |  \n",
      " |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      " |  \n",
      " |  This estimator implements regularized linear models with stochastic\n",
      " |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      " |  each sample at a time and the model is updated along the way with a\n",
      " |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      " |  (online/out-of-core) learning via the `partial_fit` method.\n",
      " |  For best results using the default learning rate schedule, the data should\n",
      " |  have zero mean and unit variance.\n",
      " |  \n",
      " |  This implementation works with data represented as dense or sparse arrays\n",
      " |  of floating point values for the features. The model it fits can be\n",
      " |  controlled with the loss parameter; by default, it fits a linear support\n",
      " |  vector machine (SVM).\n",
      " |  \n",
      " |  The regularizer is a penalty added to the loss function that shrinks model\n",
      " |  parameters towards the zero vector using either the squared euclidean norm\n",
      " |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      " |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      " |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      " |  online feature selection.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <sgd>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : str, default='hinge'\n",
      " |      The loss function to be used. Defaults to 'hinge', which gives a\n",
      " |      linear SVM.\n",
      " |  \n",
      " |      The possible options are 'hinge', 'log', 'modified_huber',\n",
      " |      'squared_hinge', 'perceptron', or a regression loss: 'squared_loss',\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |  \n",
      " |      The 'log' loss gives logistic regression, a probabilistic classifier.\n",
      " |      'modified_huber' is another smooth loss that brings tolerance to\n",
      " |      outliers as well as probability estimates.\n",
      " |      'squared_hinge' is like hinge but is quadratically penalized.\n",
      " |      'perceptron' is the linear loss used by the perceptron algorithm.\n",
      " |      The other losses are designed for regression but can be useful in\n",
      " |      classification as well; see\n",
      " |      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      " |  \n",
      " |      More details about the losses formulas can be found in the\n",
      " |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      " |  \n",
      " |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      " |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      " |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      " |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      " |      not achievable with 'l2'.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Constant that multiplies the regularization term. The higher the\n",
      " |      value, the stronger the regularization.\n",
      " |      Also used to compute the learning rate when set to `learning_rate` is\n",
      " |      set to 'optimal'.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.15\n",
      " |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      " |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      " |      Only used if `penalty` is 'elasticnet'.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If False, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of passes over the training data (aka epochs).\n",
      " |      It only impacts the behavior in the ``fit`` method, and not the\n",
      " |      :meth:`partial_fit` method.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      The stopping criterion. If it is not None, training will stop\n",
      " |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      " |      epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether or not the training data should be shuffled after each epoch.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      The verbosity level.\n",
      " |  \n",
      " |  epsilon : float, default=0.1\n",
      " |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |      For 'huber', determines the threshold at which it becomes less\n",
      " |      important to get the prediction exactly right.\n",
      " |      For epsilon-insensitive, any differences between the current prediction\n",
      " |      and the correct label are ignored if they are less than this threshold.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      " |      multi-class problems) computation.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  learning_rate : str, default='optimal'\n",
      " |      The learning rate schedule:\n",
      " |  \n",
      " |      - 'constant': `eta = eta0`\n",
      " |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      " |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      " |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      " |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      " |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      " |        training loss by tol or fail to increase validation score by tol if\n",
      " |        early_stopping is True, the current learning rate is divided by 5.\n",
      " |  \n",
      " |          .. versionadded:: 0.20\n",
      " |              Added 'adaptive' option\n",
      " |  \n",
      " |  eta0 : double, default=0.0\n",
      " |      The initial learning rate for the 'constant', 'invscaling' or\n",
      " |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      " |      the default schedule 'optimal'.\n",
      " |  \n",
      " |  power_t : double, default=0.5\n",
      " |      The exponent for inverse scaling learning rate [default 0.5].\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to True, it will automatically set aside\n",
      " |      a stratified fraction of training data as validation and terminate\n",
      " |      training when validation score returned by the `score` method is not\n",
      " |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'early_stopping' option\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if `early_stopping` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'validation_fraction' option\n",
      " |  \n",
      " |  n_iter_no_change : int, default=5\n",
      " |      Number of iterations with no improvement to wait before early stopping.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'n_iter_no_change' option\n",
      " |  \n",
      " |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      " |      Preset for the class_weight fit parameter.\n",
      " |  \n",
      " |      Weights associated with classes. If not given, all classes\n",
      " |      are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      " |      result in a different solution than when calling fit a single time\n",
      " |      because of the way the data is shuffled.\n",
      " |      If a dynamic learning rate is used, the learning rate is adapted\n",
      " |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      " |      this counter, while ``partial_fit`` will result in increasing the\n",
      " |      existing counter.\n",
      " |  \n",
      " |  average : bool or int, default=False\n",
      " |      When set to True, computes the averaged SGD weights accross all\n",
      " |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      " |      an int greater than 1, averaging will begin once the total number of\n",
      " |      samples seen reaches `average`. So ``average=10`` will begin\n",
      " |      averaging after seeing 10 samples.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      " |      Weights assigned to the features.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations before reaching the stopping criterion.\n",
      " |      For multiclass fits, it is the maximum over every binary fit.\n",
      " |  \n",
      " |  loss_function_ : concrete ``LossFunction``\n",
      " |  \n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |  \n",
      " |  t_ : int\n",
      " |      Number of weight updates performed during training.\n",
      " |      Same as ``(n_iter_ * n_samples)``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.svm.LinearSVC: Linear support vector classification.\n",
      " |  LogisticRegression: Logistic regression.\n",
      " |  Perceptron: Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      " |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      " |      penalty=None)``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import SGDClassifier\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> Y = np.array([1, 1, 2, 2])\n",
      " |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      " |  >>> clf = make_pipeline(StandardScaler(),\n",
      " |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('sgdclassifier', SGDClassifier())])\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGDClassifier\n",
      " |      BaseSGDClassifier\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseSGD\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Log of probability estimates.\n",
      " |      \n",
      " |      This method is only available for log loss and modified Huber loss.\n",
      " |      \n",
      " |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      " |      and ones, so taking the logarithm is not possible.\n",
      " |      \n",
      " |      See ``predict_proba`` for details.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Input data for prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in\n",
      " |          `self.classes_`.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      This method is only available for log loss and modified Huber loss.\n",
      " |      \n",
      " |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      " |      estimates by simple normalization, as recommended by Zadrozny and\n",
      " |      Elkan.\n",
      " |      \n",
      " |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      " |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      " |      it is necessary to perform proper probability calibration by wrapping\n",
      " |      the classifier with\n",
      " |      :class:`sklearn.calibration.CalibratedClassifierCV` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Input data for prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in `self.classes_`.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      " |      probability estimates\", SIGKDD'02,\n",
      " |      http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf\n",
      " |      \n",
      " |      The justification for the formula in the loss=\"modified_huber\"\n",
      " |      case is in the appendix B in:\n",
      " |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGDClassifier:\n",
      " |  \n",
      " |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      " |      Fit linear model with Stochastic Gradient Descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      " |          The initial coefficients to warm-start the optimization.\n",
      " |      \n",
      " |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      " |          The initial intercept to warm-start the optimization.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed. These weights will\n",
      " |          be multiplied with class_weight (passed through the\n",
      " |          constructor) if class_weight is specified.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Perform one epoch of stochastic gradient descent on given samples.\n",
      " |      \n",
      " |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      " |      guaranteed that a minimum of the cost function is reached after calling\n",
      " |      it once. Matters such as objective convergence and early stopping\n",
      " |      should be handled by the user.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Subset of the training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Subset of the target values.\n",
      " |      \n",
      " |      classes : ndarray of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseSGDClassifier:\n",
      " |  \n",
      " |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGD:\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set and validate the parameters of estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSGD:\n",
      " |  \n",
      " |  average_coef_\n",
      " |  \n",
      " |  average_intercept_\n",
      " |  \n",
      " |  standard_coef_\n",
      " |  \n",
      " |  standard_intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SGDClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clf, y_clf = make_classification(n_samples=100000, random_state=1234)\n",
    "gen_clf = BatchGenerator(x_clf, y_clf, batch_size=100, random_state=1234)\n",
    "clf = SGDClassifier(loss='log', n_jobs=-1, random_state=1234)\n",
    "classes = np.unique(y_clf)\n",
    "for _ in range(10000):\n",
    "    batch_x, batch_y = next(gen_clf)\n",
    "    clf.partial_fit(batch_x, batch_y, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Regressor\n",
    "---\n",
    "確率的勾配降下法 (SGD) による回帰。  \n",
    "`sklearn.linear_model.SGDRegressor`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGDRegressor in module sklearn.linear_model._stochastic_gradient:\n",
      "\n",
      "class SGDRegressor(BaseSGDRegressor)\n",
      " |  SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      " |  \n",
      " |  Linear model fitted by minimizing a regularized empirical loss with SGD\n",
      " |  \n",
      " |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      " |  estimated each sample at a time and the model is updated along the way with\n",
      " |  a decreasing strength schedule (aka learning rate).\n",
      " |  \n",
      " |  The regularizer is a penalty added to the loss function that shrinks model\n",
      " |  parameters towards the zero vector using either the squared euclidean norm\n",
      " |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      " |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      " |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      " |  online feature selection.\n",
      " |  \n",
      " |  This implementation works with data represented as dense numpy arrays of\n",
      " |  floating point values for the features.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <sgd>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : str, default='squared_loss'\n",
      " |      The loss function to be used. The possible values are 'squared_loss',\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      " |  \n",
      " |      The 'squared_loss' refers to the ordinary least squares fit.\n",
      " |      'huber' modifies 'squared_loss' to focus less on getting outliers\n",
      " |      correct by switching from squared to linear loss past a distance of\n",
      " |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      " |      linear past that; this is the loss function used in SVR.\n",
      " |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      " |      a tolerance of epsilon.\n",
      " |  \n",
      " |      More details about the losses formulas can be found in the\n",
      " |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      " |  \n",
      " |  penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
      " |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      " |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      " |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      " |      not achievable with 'l2'.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Constant that multiplies the regularization term. The higher the\n",
      " |      value, the stronger the regularization.\n",
      " |      Also used to compute the learning rate when set to `learning_rate` is\n",
      " |      set to 'optimal'.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.15\n",
      " |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      " |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      " |      Only used if `penalty` is 'elasticnet'.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If False, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of passes over the training data (aka epochs).\n",
      " |      It only impacts the behavior in the ``fit`` method, and not the\n",
      " |      :meth:`partial_fit` method.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      The stopping criterion. If it is not None, training will stop\n",
      " |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      " |      epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether or not the training data should be shuffled after each epoch.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      The verbosity level.\n",
      " |  \n",
      " |  epsilon : float, default=0.1\n",
      " |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      " |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      " |      For 'huber', determines the threshold at which it becomes less\n",
      " |      important to get the prediction exactly right.\n",
      " |      For epsilon-insensitive, any differences between the current prediction\n",
      " |      and the correct label are ignored if they are less than this threshold.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  learning_rate : string, default='invscaling'\n",
      " |      The learning rate schedule:\n",
      " |  \n",
      " |      - 'constant': `eta = eta0`\n",
      " |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      " |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      " |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      " |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      " |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      " |        training loss by tol or fail to increase validation score by tol if\n",
      " |        early_stopping is True, the current learning rate is divided by 5.\n",
      " |  \n",
      " |          .. versionadded:: 0.20\n",
      " |              Added 'adaptive' option\n",
      " |  \n",
      " |  eta0 : double, default=0.01\n",
      " |      The initial learning rate for the 'constant', 'invscaling' or\n",
      " |      'adaptive' schedules. The default value is 0.01.\n",
      " |  \n",
      " |  power_t : double, default=0.25\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to True, it will automatically set aside\n",
      " |      a fraction of training data as validation and terminate\n",
      " |      training when validation score returned by the `score` method is not\n",
      " |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      " |      epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'early_stopping' option\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if `early_stopping` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'validation_fraction' option\n",
      " |  \n",
      " |  n_iter_no_change : int, default=5\n",
      " |      Number of iterations with no improvement to wait before early stopping.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |          Added 'n_iter_no_change' option\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      " |      result in a different solution than when calling fit a single time\n",
      " |      because of the way the data is shuffled.\n",
      " |      If a dynamic learning rate is used, the learning rate is adapted\n",
      " |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      " |      this counter, while ``partial_fit``  will result in increasing the\n",
      " |      existing counter.\n",
      " |  \n",
      " |  average : bool or int, default=False\n",
      " |      When set to True, computes the averaged SGD weights accross all\n",
      " |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      " |      an int greater than 1, averaging will begin once the total number of\n",
      " |      samples seen reaches `average`. So ``average=10`` will begin\n",
      " |      averaging after seeing 10 samples.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features,)\n",
      " |      Weights assigned to the features.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,)\n",
      " |      The intercept term.\n",
      " |  \n",
      " |  average_coef_ : ndarray of shape (n_features,)\n",
      " |      Averaged weights assigned to the features. Only available\n",
      " |      if ``average=True``.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          Attribute ``average_coef_`` was deprecated\n",
      " |          in version 0.23 and will be removed in 0.25.\n",
      " |  \n",
      " |  average_intercept_ : ndarray of shape (1,)\n",
      " |      The averaged intercept term. Only available if ``average=True``.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          Attribute ``average_intercept_`` was deprecated\n",
      " |          in version 0.23 and will be removed in 0.25.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations before reaching the stopping criterion.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      Number of weight updates performed during training.\n",
      " |      Same as ``(n_iter_ * n_samples)``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import SGDRegressor\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> n_samples, n_features = 10, 5\n",
      " |  >>> rng = np.random.RandomState(0)\n",
      " |  >>> y = rng.randn(n_samples)\n",
      " |  >>> X = rng.randn(n_samples, n_features)\n",
      " |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      " |  >>> reg = make_pipeline(StandardScaler(),\n",
      " |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      " |  >>> reg.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('sgdregressor', SGDRegressor())])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  Ridge, ElasticNet, Lasso, sklearn.svm.SVR\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGDRegressor\n",
      " |      BaseSGDRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseSGD\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loss='squared_loss', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGDRegressor:\n",
      " |  \n",
      " |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      " |      Fit linear model with Stochastic Gradient Descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_features,), default=None\n",
      " |          The initial coefficients to warm-start the optimization.\n",
      " |      \n",
      " |      intercept_init : ndarray of shape (1,), default=None\n",
      " |          The initial intercept to warm-start the optimization.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, sample_weight=None)\n",
      " |      Perform one epoch of stochastic gradient descent on given samples.\n",
      " |      \n",
      " |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      " |      guaranteed that a minimum of the cost function is reached after calling\n",
      " |      it once. Matters such as objective convergence and early stopping\n",
      " |      should be handled by the user.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Subset of training data\n",
      " |      \n",
      " |      y : numpy array of shape (n_samples,)\n",
      " |          Subset of target values\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ndarray of shape (n_samples,)\n",
      " |         Predicted target values per element in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseSGDRegressor:\n",
      " |  \n",
      " |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSGD:\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set and validate the parameters of estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseSGD:\n",
      " |  \n",
      " |  average_coef_\n",
      " |  \n",
      " |  average_intercept_\n",
      " |  \n",
      " |  standard_coef_\n",
      " |  \n",
      " |  standard_intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SGDRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reg, y_reg = make_regression(n_samples=100000, random_state=1234)\n",
    "gen_reg = BatchGenerator(x_reg, y_reg, batch_size=100, random_state=1234)\n",
    "reg = SGDRegressor(random_state=1234)\n",
    "for _ in range(10000):\n",
    "    batch_x, batch_y = next(gen_reg)\n",
    "    reg.partial_fit(batch_x, batch_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
