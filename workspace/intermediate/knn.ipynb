{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.neighbors import KNeighborsClassifier, DistanceMetric\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "pd.set_option('max_rows', 5)\n",
    "pd.set_option('max_columns', 9)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k近傍法 (k-nearest neighbors)\n",
    "---\n",
    "特徴空間内で距離的に近い教師データに基づいて分類するモデル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width    species\n",
       "0             5.1          3.5     setosa\n",
       "1             4.9          3.0     setosa\n",
       "..            ...          ...        ...\n",
       "148           6.2          3.4  virginica\n",
       "149           5.9          3.0  virginica\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris')[['sepal_length', 'sepal_width', 'species']]\n",
    "iris['species'] = pd.Categorical(iris['species'])\n",
    "print('iris')\n",
    "display(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b446c693e79b446faab06cba583afc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, continuous_update=False, description='k', max=10, min=1), Output()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_functions.knn import algorithm\n",
    "algorithm.show(iris.iloc[:, :2].values, iris['species'].cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仕組み\n",
    "---\n",
    "- 分類したいデータから特徴空間内での距離の近いk個の教師データの多数決で分類を決定\n",
    "- 訓練データからパラメータを学習せず、訓練データを暗記する怠惰学習(lazy learner)の代表例\n",
    "- 学習にかかるコストはないが、訓練データが増えると記憶容量・計算量が問題になる\n",
    "- 特徴が増えるに従って、次元の呪い (計算量が爆発的に増えたり、精度が極端に悪化すること) に陥りやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`wine`データセットのサンプル間距離を算出 (`scipy.spatial.distance.pdist`と`scipy.spatial.distance.squareform`を使用) し、それぞれのサンプルから最も近い 3 サンプルの`target`ラベルに基づいて、予測クラスを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>...</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  ...   hue  \\\n",
       "0      14.23        1.71  2.43               15.6  ...  1.04   \n",
       "1      13.20        1.78  2.14               11.2  ...  1.05   \n",
       "..       ...         ...   ...                ...  ...   ...   \n",
       "176    13.17        2.59  2.37               20.0  ...  0.60   \n",
       "177    14.13        4.10  2.74               24.5  ...  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0     0.0  \n",
       "1                            3.40   1050.0     0.0  \n",
       "..                            ...      ...     ...  \n",
       "176                          1.62    840.0     2.0  \n",
       "177                          1.60    560.0     2.0  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = load_wine()\n",
    "wine = pd.DataFrame(np.column_stack([loader.data, loader.target]),\n",
    "                    columns=loader.feature_names + ['target'])\n",
    "print('wine')\n",
    "display(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pdist in module scipy.spatial.distance:\n",
      "\n",
      "pdist(X, metric='euclidean', *args, **kwargs)\n",
      "    Pairwise distances between observations in n-dimensional space.\n",
      "    \n",
      "    See Notes for common calling conventions.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : ndarray\n",
      "        An m by n array of m original observations in an\n",
      "        n-dimensional space.\n",
      "    metric : str or function, optional\n",
      "        The distance metric to use. The distance function can\n",
      "        be 'braycurtis', 'canberra', 'chebyshev', 'cityblock',\n",
      "        'correlation', 'cosine', 'dice', 'euclidean', 'hamming',\n",
      "        'jaccard', 'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n",
      "        'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "        'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'.\n",
      "    *args : tuple. Deprecated.\n",
      "        Additional arguments should be passed as keyword arguments\n",
      "    **kwargs : dict, optional\n",
      "        Extra arguments to `metric`: refer to each metric documentation for a\n",
      "        list of all possible arguments.\n",
      "    \n",
      "        Some possible arguments:\n",
      "    \n",
      "        p : scalar\n",
      "        The p-norm to apply for Minkowski, weighted and unweighted.\n",
      "        Default: 2.\n",
      "    \n",
      "        w : ndarray\n",
      "        The weight vector for metrics that support weights (e.g., Minkowski).\n",
      "    \n",
      "        V : ndarray\n",
      "        The variance vector for standardized Euclidean.\n",
      "        Default: var(X, axis=0, ddof=1)\n",
      "    \n",
      "        VI : ndarray\n",
      "        The inverse of the covariance matrix for Mahalanobis.\n",
      "        Default: inv(cov(X.T)).T\n",
      "    \n",
      "        out : ndarray.\n",
      "        The output array\n",
      "        If not None, condensed distance matrix Y is stored in this array.\n",
      "        Note: metric independent, it will become a regular keyword arg in a\n",
      "        future scipy version\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Y : ndarray\n",
      "        Returns a condensed distance matrix Y.  For\n",
      "        each :math:`i` and :math:`j` (where :math:`i<j<m`),where m is the number\n",
      "        of original observations. The metric ``dist(u=X[i], v=X[j])``\n",
      "        is computed and stored in entry ``ij``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    squareform : converts between condensed distance matrices and\n",
      "                 square distance matrices.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    See ``squareform`` for information on how to calculate the index of\n",
      "    this entry or to convert the condensed distance matrix to a\n",
      "    redundant square matrix.\n",
      "    \n",
      "    The following are common calling conventions.\n",
      "    \n",
      "    1. ``Y = pdist(X, 'euclidean')``\n",
      "    \n",
      "       Computes the distance between m points using Euclidean distance\n",
      "       (2-norm) as the distance metric between the points. The points\n",
      "       are arranged as m n-dimensional row vectors in the matrix X.\n",
      "    \n",
      "    2. ``Y = pdist(X, 'minkowski', p=2.)``\n",
      "    \n",
      "       Computes the distances using the Minkowski distance\n",
      "       :math:`||u-v||_p` (p-norm) where :math:`p \\geq 1`.\n",
      "    \n",
      "    3. ``Y = pdist(X, 'cityblock')``\n",
      "    \n",
      "       Computes the city block or Manhattan distance between the\n",
      "       points.\n",
      "    \n",
      "    4. ``Y = pdist(X, 'seuclidean', V=None)``\n",
      "    \n",
      "       Computes the standardized Euclidean distance. The standardized\n",
      "       Euclidean distance between two n-vectors ``u`` and ``v`` is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          \\sqrt{\\sum {(u_i-v_i)^2 / V[x_i]}}\n",
      "    \n",
      "    \n",
      "       V is the variance vector; V[i] is the variance computed over all\n",
      "       the i'th components of the points.  If not passed, it is\n",
      "       automatically computed.\n",
      "    \n",
      "    5. ``Y = pdist(X, 'sqeuclidean')``\n",
      "    \n",
      "       Computes the squared Euclidean distance :math:`||u-v||_2^2` between\n",
      "       the vectors.\n",
      "    \n",
      "    6. ``Y = pdist(X, 'cosine')``\n",
      "    \n",
      "       Computes the cosine distance between vectors u and v,\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          1 - \\frac{u \\cdot v}\n",
      "                   {{||u||}_2 {||v||}_2}\n",
      "    \n",
      "       where :math:`||*||_2` is the 2-norm of its argument ``*``, and\n",
      "       :math:`u \\cdot v` is the dot product of ``u`` and ``v``.\n",
      "    \n",
      "    7. ``Y = pdist(X, 'correlation')``\n",
      "    \n",
      "       Computes the correlation distance between vectors u and v. This is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          1 - \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}\n",
      "                   {{||(u - \\bar{u})||}_2 {||(v - \\bar{v})||}_2}\n",
      "    \n",
      "       where :math:`\\bar{v}` is the mean of the elements of vector v,\n",
      "       and :math:`x \\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
      "    \n",
      "    8. ``Y = pdist(X, 'hamming')``\n",
      "    \n",
      "       Computes the normalized Hamming distance, or the proportion of\n",
      "       those vector elements between two n-vectors ``u`` and ``v``\n",
      "       which disagree. To save memory, the matrix ``X`` can be of type\n",
      "       boolean.\n",
      "    \n",
      "    9. ``Y = pdist(X, 'jaccard')``\n",
      "    \n",
      "       Computes the Jaccard distance between the points. Given two\n",
      "       vectors, ``u`` and ``v``, the Jaccard distance is the\n",
      "       proportion of those elements ``u[i]`` and ``v[i]`` that\n",
      "       disagree.\n",
      "    \n",
      "    10. ``Y = pdist(X, 'chebyshev')``\n",
      "    \n",
      "       Computes the Chebyshev distance between the points. The\n",
      "       Chebyshev distance between two n-vectors ``u`` and ``v`` is the\n",
      "       maximum norm-1 distance between their respective elements. More\n",
      "       precisely, the distance is given by\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          d(u,v) = \\max_i {|u_i-v_i|}\n",
      "    \n",
      "    11. ``Y = pdist(X, 'canberra')``\n",
      "    \n",
      "       Computes the Canberra distance between the points. The\n",
      "       Canberra distance between two points ``u`` and ``v`` is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "         d(u,v) = \\sum_i \\frac{|u_i-v_i|}\n",
      "                              {|u_i|+|v_i|}\n",
      "    \n",
      "    \n",
      "    12. ``Y = pdist(X, 'braycurtis')``\n",
      "    \n",
      "       Computes the Bray-Curtis distance between the points. The\n",
      "       Bray-Curtis distance between two points ``u`` and ``v`` is\n",
      "    \n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "            d(u,v) = \\frac{\\sum_i {|u_i-v_i|}}\n",
      "                           {\\sum_i {|u_i+v_i|}}\n",
      "    \n",
      "    13. ``Y = pdist(X, 'mahalanobis', VI=None)``\n",
      "    \n",
      "       Computes the Mahalanobis distance between the points. The\n",
      "       Mahalanobis distance between two points ``u`` and ``v`` is\n",
      "       :math:`\\sqrt{(u-v)(1/V)(u-v)^T}` where :math:`(1/V)` (the ``VI``\n",
      "       variable) is the inverse covariance. If ``VI`` is not None,\n",
      "       ``VI`` will be used as the inverse covariance matrix.\n",
      "    \n",
      "    14. ``Y = pdist(X, 'yule')``\n",
      "    \n",
      "       Computes the Yule distance between each pair of boolean\n",
      "       vectors. (see yule function documentation)\n",
      "    \n",
      "    15. ``Y = pdist(X, 'matching')``\n",
      "    \n",
      "       Synonym for 'hamming'.\n",
      "    \n",
      "    16. ``Y = pdist(X, 'dice')``\n",
      "    \n",
      "       Computes the Dice distance between each pair of boolean\n",
      "       vectors. (see dice function documentation)\n",
      "    \n",
      "    17. ``Y = pdist(X, 'kulsinski')``\n",
      "    \n",
      "       Computes the Kulsinski distance between each pair of\n",
      "       boolean vectors. (see kulsinski function documentation)\n",
      "    \n",
      "    18. ``Y = pdist(X, 'rogerstanimoto')``\n",
      "    \n",
      "       Computes the Rogers-Tanimoto distance between each pair of\n",
      "       boolean vectors. (see rogerstanimoto function documentation)\n",
      "    \n",
      "    19. ``Y = pdist(X, 'russellrao')``\n",
      "    \n",
      "       Computes the Russell-Rao distance between each pair of\n",
      "       boolean vectors. (see russellrao function documentation)\n",
      "    \n",
      "    20. ``Y = pdist(X, 'sokalmichener')``\n",
      "    \n",
      "       Computes the Sokal-Michener distance between each pair of\n",
      "       boolean vectors. (see sokalmichener function documentation)\n",
      "    \n",
      "    21. ``Y = pdist(X, 'sokalsneath')``\n",
      "    \n",
      "       Computes the Sokal-Sneath distance between each pair of\n",
      "       boolean vectors. (see sokalsneath function documentation)\n",
      "    \n",
      "    22. ``Y = pdist(X, 'wminkowski', p=2, w=w)``\n",
      "    \n",
      "       Computes the weighted Minkowski distance between each pair of\n",
      "       vectors. (see wminkowski function documentation)\n",
      "    \n",
      "    23. ``Y = pdist(X, f)``\n",
      "    \n",
      "       Computes the distance between all pairs of vectors in X\n",
      "       using the user supplied 2-arity function f. For example,\n",
      "       Euclidean distance between the vectors could be computed\n",
      "       as follows::\n",
      "    \n",
      "         dm = pdist(X, lambda u, v: np.sqrt(((u-v)**2).sum()))\n",
      "    \n",
      "       Note that you should avoid passing a reference to one of\n",
      "       the distance functions defined in this library. For example,::\n",
      "    \n",
      "         dm = pdist(X, sokalsneath)\n",
      "    \n",
      "       would calculate the pair-wise distances between the vectors in\n",
      "       X using the Python function sokalsneath. This would result in\n",
      "       sokalsneath being called :math:`{n \\choose 2}` times, which\n",
      "       is inefficient. Instead, the optimized C version is more\n",
      "       efficient, and we call it using the following syntax.::\n",
      "    \n",
      "         dm = pdist(X, 'sokalsneath')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function squareform in module scipy.spatial.distance:\n",
      "\n",
      "squareform(X, force='no', checks=True)\n",
      "    Convert a vector-form distance vector to a square-form distance\n",
      "    matrix, and vice-versa.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : ndarray\n",
      "        Either a condensed or redundant distance matrix.\n",
      "    force : str, optional\n",
      "        As with MATLAB(TM), if force is equal to ``'tovector'`` or\n",
      "        ``'tomatrix'``, the input will be treated as a distance matrix or\n",
      "        distance vector respectively.\n",
      "    checks : bool, optional\n",
      "        If set to False, no checks will be made for matrix\n",
      "        symmetry nor zero diagonals. This is useful if it is known that\n",
      "        ``X - X.T1`` is small and ``diag(X)`` is close to zero.\n",
      "        These values are ignored any way so they do not disrupt the\n",
      "        squareform transformation.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Y : ndarray\n",
      "        If a condensed distance matrix is passed, a redundant one is\n",
      "        returned, or if a redundant one is passed, a condensed distance\n",
      "        matrix is returned.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    1. v = squareform(X)\n",
      "    \n",
      "       Given a square d-by-d symmetric distance matrix X,\n",
      "       ``v = squareform(X)`` returns a ``d * (d-1) / 2`` (or\n",
      "       :math:`{n \\choose 2}`) sized vector v.\n",
      "    \n",
      "      :math:`v[{n \\choose 2}-{n-i \\choose 2} + (j-i-1)]` is the distance\n",
      "      between points i and j. If X is non-square or asymmetric, an error\n",
      "      is returned.\n",
      "    \n",
      "    2. X = squareform(v)\n",
      "    \n",
      "      Given a ``d*(d-1)/2`` sized v for some integer ``d >= 2`` encoding\n",
      "      distances as described, ``X = squareform(v)`` returns a d by d distance\n",
      "      matrix X.  The ``X[i, j]`` and ``X[j, i]`` values are set to\n",
      "      :math:`v[{n \\choose 2}-{n-i \\choose 2} + (j-i-1)]` and all\n",
      "      diagonal elements are zero.\n",
      "    \n",
      "    In SciPy 0.19.0, ``squareform`` stopped casting all input types to\n",
      "    float64, and started returning arrays of the same dtype as the input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(squareform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率 : 1.0\n"
     ]
    }
   ],
   "source": [
    "dist_matrix = squareform(pdist(wine.iloc[:, :-1].values))\n",
    "nearests3 = pd.DataFrame(np.argsort(dist_matrix, axis=1)[:, :3])\n",
    "pred = stats.mode(nearests3.applymap(lambda i: wine.iloc[i, -1]),\n",
    "                  axis=1).mode[:, 0]\n",
    "\n",
    "# scikit-learnの結果と比較\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(wine.iloc[:, :-1], wine['target'])\n",
    "print(f'正解率 : {sum(pred == knn.predict(wine.iloc[:, :-1])) / wine.index.size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次元の呪い (高次元空間内の距離)\n",
    "---\n",
    "$1$ 次元空間 (直線) 上では変数の値が $1$ 増えると、その間の距離は $1$ 。  \n",
    "$2$ 次元空間 (平面) 上では変数の値が $1$ ずつ増えると、その間の距離は $\\sqrt{2}$ 。 (元の点から距離 $1$ の点の集合は半径 $1$ の円)  \n",
    "$3$ 次元空間 (空間) 上では変数の値が $1$ ずつ増えると、その間の距離は $\\sqrt{3}$ 。 (元の点から距離 $1$ の点の集合は半径 $1$ の球)  \n",
    "$\\vdots$  \n",
    "$n$ 次元空間 (超空間) 上では変数の値が $1$ ずつ増えると、その間の距離は $\\sqrt{n}$ 。\n",
    "\n",
    "以上のように、変数が増えるにしたがって、それぞれの変数のわずかな変化によってサンプル同士がいずれのサンプルからも遠くなってしまい、近いサンプルという考え方の妥当性が低くなる。これが次元の呪いの原因の一つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonでのk近傍法の実行方法\n",
    "---\n",
    "`sklearn.neighbors.KNeighborsClassifier`を使用する。使用できる距離は[`sklearn.neighbors.DistanceMetric`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsClassifier in module sklearn.neighbors._classification:\n",
      "\n",
      "class KNeighborsClassifier(sklearn.neighbors._base.NeighborsBase, sklearn.neighbors._base.KNeighborsMixin, sklearn.neighbors._base.SupervisedIntegerMixin, sklearn.base.ClassifierMixin)\n",
      " |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |  \n",
      " |  Classifier implementing the k-nearest neighbors vote.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      " |      list of available metrics.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`,\n",
      " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  effective_metric_ : str or callble\n",
      " |      The distance metric used. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  outputs_2d_ : bool\n",
      " |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      " |      otherwise True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsClassifier(...)\n",
      " |  >>> print(neigh.predict([[1.1]]))\n",
      " |  [0]\n",
      " |  >>> print(neigh.predict_proba([[0.9]]))\n",
      " |  [[0.66666667 0.33333333]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  NearestNeighbors\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsClassifier\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.neighbors._base.SupervisedIntegerMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the class labels for the provided data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      " |          Class labels for each data sample.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n",
      " |          of such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. Classes are ordered\n",
      " |          by lexicographic order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors to get (default is the value\n",
      " |          passed to the constructor).\n",
      " |      \n",
      " |      return_distance : boolean, optional. Defaults to True.\n",
      " |          If False, distances will not be returned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : array, shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      neigh_ind : array, shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int\n",
      " |          Number of neighbors for each sample.\n",
      " |          (default is value passed to the constructor).\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, optional\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse graph in CSR format, shape = [n_queries, n_samples_fit]\n",
      " |          n_samples_fit is the number of samples in the fitted data\n",
      " |          A[i, j] is assigned the weight of edge that connects i to j.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.SupervisedIntegerMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model using X as training data and y as target values\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, BallTree, KDTree}\n",
      " |          Training data. If array or matrix, shape [n_samples, n_features],\n",
      " |          or [n_samples, n_samples] if metric='precomputed'.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix}\n",
      " |          Target values of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DistanceMetric in module sklearn.neighbors._dist_metrics:\n",
      "\n",
      "class DistanceMetric(builtins.object)\n",
      " |  DistanceMetric class\n",
      " |  \n",
      " |  This class provides a uniform interface to fast distance metric\n",
      " |  functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
      " |  class method and the metric string identifier (see below).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neighbors import DistanceMetric\n",
      " |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      " |  >>> X = [[0, 1, 2],\n",
      " |           [3, 4, 5]]\n",
      " |  >>> dist.pairwise(X)\n",
      " |  array([[ 0.        ,  5.19615242],\n",
      " |         [ 5.19615242,  0.        ]])\n",
      " |  \n",
      " |  Available Metrics\n",
      " |  \n",
      " |  The following lists the string metric identifiers and the associated\n",
      " |  distance metric classes:\n",
      " |  \n",
      " |  **Metrics intended for real-valued vector spaces:**\n",
      " |  \n",
      " |  ==============  ====================  ========  ===============================\n",
      " |  identifier      class name            args      distance function\n",
      " |  --------------  --------------------  --------  -------------------------------\n",
      " |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      " |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      " |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      " |  \"minkowski\"     MinkowskiDistance     p         ``sum(|x - y|^p)^(1/p)``\n",
      " |  \"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
      " |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      " |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      " |  ==============  ====================  ========  ===============================\n",
      " |  \n",
      " |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      " |  distance metric requires data in the form of [latitude, longitude] and both\n",
      " |  inputs and outputs are in units of radians.\n",
      " |  \n",
      " |  ============  ==================  ===============================================================\n",
      " |  identifier    class name          distance function\n",
      " |  ------------  ------------------  ---------------------------------------------------------------\n",
      " |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      " |  ============  ==================  ===============================================================\n",
      " |  \n",
      " |  \n",
      " |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      " |  for integer-valued vectors, these are also valid metrics in the case of\n",
      " |  real-valued vectors.\n",
      " |  \n",
      " |  =============  ====================  ========================================\n",
      " |  identifier     class name            distance function\n",
      " |  -------------  --------------------  ----------------------------------------\n",
      " |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      " |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      " |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      " |  =============  ====================  ========================================\n",
      " |  \n",
      " |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      " |  is evaluated to \"True\".  In the listings below, the following\n",
      " |  abbreviations are used:\n",
      " |  \n",
      " |   - N  : number of dimensions\n",
      " |   - NTT : number of dims in which both values are True\n",
      " |   - NTF : number of dims in which the first value is True, second is False\n",
      " |   - NFT : number of dims in which the first value is False, second is True\n",
      " |   - NFF : number of dims in which both values are False\n",
      " |   - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
      " |   - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      " |  \n",
      " |  =================  =======================  ===============================\n",
      " |  identifier         class name               distance function\n",
      " |  -----------------  -----------------------  -------------------------------\n",
      " |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      " |  \"matching\"         MatchingDistance         NNEQ / N\n",
      " |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      " |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      " |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      " |  \"russellrao\"       RussellRaoDistance       NNZ / N\n",
      " |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      " |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      " |  =================  =======================  ===============================\n",
      " |  \n",
      " |  **User-defined distance:**\n",
      " |  \n",
      " |  ===========    ===============    =======\n",
      " |  identifier     class name         args\n",
      " |  -----------    ---------------    -------\n",
      " |  \"pyfunc\"       PyFuncDistance     func\n",
      " |  ===========    ===============    =======\n",
      " |  \n",
      " |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      " |  arrays, and returns a distance.  Note that in order to be used within\n",
      " |  the BallTree, the distance must be a true metric:\n",
      " |  i.e. it must satisfy the following properties\n",
      " |  \n",
      " |  1) Non-negativity: d(x, y) >= 0\n",
      " |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      " |  3) Symmetry: d(x, y) = d(y, x)\n",
      " |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      " |  \n",
      " |  Because of the Python object overhead involved in calling the python\n",
      " |  function, this will be fairly slow, but it will have the same\n",
      " |  scaling as other distances.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(...)\n",
      " |      get state for pickling\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      reduce method used for pickling\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      set state for pickling\n",
      " |  \n",
      " |  dist_to_rdist(...)\n",
      " |      Convert the true distance to the reduced distance.\n",
      " |      \n",
      " |      The reduced distance, defined for some metrics, is a computationally\n",
      " |      more efficient measure which preserves the rank of the true distance.\n",
      " |      For example, in the Euclidean distance metric, the reduced distance\n",
      " |      is the squared-euclidean distance.\n",
      " |  \n",
      " |  pairwise(...)\n",
      " |      Compute the pairwise distances between X and Y\n",
      " |      \n",
      " |      This is a convenience routine for the sake of testing.  For many\n",
      " |      metrics, the utilities in scipy.spatial.distance.cdist and\n",
      " |      scipy.spatial.distance.pdist will be faster.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Array of shape (Nx, D), representing Nx points in D dimensions.\n",
      " |      Y : array_like (optional)\n",
      " |          Array of shape (Ny, D), representing Ny points in D dimensions.\n",
      " |          If not specified, then Y=X.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dist : ndarray\n",
      " |          The shape (Nx, Ny) array of pairwise distances between points in\n",
      " |          X and Y.\n",
      " |  \n",
      " |  rdist_to_dist(...)\n",
      " |      Convert the Reduced distance to the true distance.\n",
      " |      \n",
      " |      The reduced distance, defined for some metrics, is a computationally\n",
      " |      more efficient measure which preserves the rank of the true distance.\n",
      " |      For example, in the Euclidean distance metric, the reduced distance\n",
      " |      is the squared-euclidean distance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_metric(...) from builtins.type\n",
      " |      Get the given distance metric from the string identifier.\n",
      " |      \n",
      " |      See the docstring of DistanceMetric for a list of available metrics.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      metric : string or class name\n",
      " |          The distance metric to use\n",
      " |      **kwargs\n",
      " |          additional arguments will be passed to the requested metric\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DistanceMetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推薦図書\n",
    "---\n",
    "- [見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑](https://www.amazon.co.jp/%E8%A6%8B%E3%81%A6%E8%A9%A6%E3%81%97%E3%81%A6%E3%82%8F%E3%81%8B%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E4%BB%95%E7%B5%84%E3%81%BF-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%9B%B3%E9%91%91-%E7%A7%8B%E5%BA%AD-%E4%BC%B8%E4%B9%9F/dp/4798155659/)\n",
    "- [Python 機械学習プログラミング 達人データサイエンティストによる理論と実践](https://www.amazon.co.jp/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E9%81%94%E4%BA%BA%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E3%81%AB%E3%82%88%E3%82%8B%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%B7%B5-impress-gear/dp/4295003379/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
