{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "pd.set_option('max_rows', 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰の回帰方程式の意味"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰の回帰方程式\n",
    "---\n",
    "$\\displaystyle z=a+b_{1} x_{1} +b_{2} x_{2} +\\cdots +b_{k} x_{k}$ とすると、ロジスティック回帰の回帰方程式は $\n",
    "\\displaystyle y=\\frac\n",
    "    {e^{z}}\n",
    "    {1+e^{z}}\n",
    "=\\frac\n",
    "    {1}\n",
    "    {1+e^{-z}}\n",
    "$ で表される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オッズとオッズ比\n",
    "---\n",
    "ある事象が起きる確率を $p$ とすると、 $\n",
    "\\displaystyle o=\\frac\n",
    "    {p}\n",
    "    {1-p}\n",
    "$ をオッズという。オッズはある事象の起こりやすさを表す。  \n",
    "また、別々のグループでのオッズ $o_{1}, o_{2}$ の比 $\n",
    "\\displaystyle \\frac\n",
    "    {o_{1}}\n",
    "    {o_{2}}\n",
    "$ をオッズ比という。オッズ比は他のグループと比較したときの相対的な事象の起こりやすさを表す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "ある調査で、男性・女性で定期的に運動している人の割合がそれぞれ $20\\% ,30\\%$ だったときに、男女それぞれのオッズとオッズ比を算出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "男性のオッズ = 0.25\n",
      "女性のオッズ = 0.43\n",
      "オッズ比 = 0.58\n"
     ]
    }
   ],
   "source": [
    "p_male, p_female = 0.2, 0.3\n",
    "o_male = p_male / (1 - p_male)\n",
    "o_female = p_female / (1 - p_female)\n",
    "o_ratio = o_male / o_female\n",
    "for sex, odds in zip(['男性', '女性'], [o_male, o_female]):\n",
    "    print(f'{sex}のオッズ = {odds:.2f}')\n",
    "print(f'オッズ比 = {o_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰方程式とオッズ\n",
    "---\n",
    "オッズ $\n",
    "\\displaystyle o=\\frac\n",
    "    {p}\n",
    "    {1-p}\n",
    "$ を $p$ について整理すると、 $\n",
    "\\displaystyle p=\\frac\n",
    "    {o}\n",
    "    {1+o}\n",
    "$ となる。  \n",
    "これをロジスティック関数 (ロジスティック分布の累積分布関数) $\n",
    "\\displaystyle f( x) =\\frac\n",
    "    {e^{x}}\n",
    "    {1+e^{x}}\n",
    "$ と比較すると、 $p$ も $f( x)$ も確率を表しているので、ロジスティック回帰の回帰方程式中の $e^{z}$ は目的変数 $y$ についてのオッズを表していると解釈できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰係数とオッズ比\n",
    "---\n",
    "説明変数 $x$ に対して回帰係数が $b$ のとき、 $x$ が 1 増加すると $z$ は $b$ 増加する。このとき $x$ の増加前後のオッズ比は $\\displaystyle \\frac{e^{z+b}}{e^{z}} =e^{b}$ なので、回帰係数 $b$ は $x$ についての対数オッズ比の推定量である。 $\\displaystyle \\left( loge^{b} =b\\right)$  \n",
    "例えば、 $x$ が男女を表すダミー変数 (男 = 1・女 = 0) だとすると、目的変数 $y$ に関して、男性のオッズは女性のオッズの $e^{b}$ 倍 (男性の対数オッズと女性の対数オッズの差は $b$) である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰方程式の求め方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尤度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "表の出る確率 $p$ が不明なコインを 10 回投げて、表が 3 回出たとする。このとき、横軸に確率 $p$ (0~1) ・縦軸に $p$ がその値のときに 10 回中 3 回表が得られる確率をとったグラフを表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.linspace(0, 1, 50)\n",
    "y = binom.pmf(3, 10, p)\n",
    "plt.plot(p, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある観測値 (サンプル) が得られたときに、そのサンプルがどのようなパラメータ (母数) の分布から得られていそうか (もっともらしいか) を表す関数を尤度 (関数) という。  \n",
    "上の練習問題のグラフは尤度を表しており、 $p=0.3$ のときに尤度が最大になる。この尤度が最大になるパラメーターの推定量を最尤推定量という。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最尤法 (Maximum Likelihood Method)\n",
    "---\n",
    "最尤法、または最尤推定 (maximum likelihood estimation) とは、最尤推定量 (尤度が最大になる母数) を母数の推定量とする方法。実際には尤度ではなく対数尤度を用いることが多い。\n",
    "\n",
    "詳細は[最尤法](maximum_likelihood_estimation.ipynb)を参照。\n",
    "\n",
    "ロジスティック回帰の場合、尤度は $\\displaystyle \\prod _{\\{i|y_{i} =1\\}}\\hat{y}_{i} \\cdot \\prod _{\\{i|y_{i} =0\\}}\\left( 1-\\hat{y}_{i}\\right)$ 、対数尤度は $\\displaystyle \\sum _{\\{i|y_{i} =1\\}} log\\hat{y}_{i} +\\sum _{\\{i|y_{i} =0\\}} log\\left( 1-\\hat{y}_{i}\\right)$ で表される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfbe2d01594452a873b01c9f94b810e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, continuous_update=False, description='$a$', max=10.0, min=-10.0),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_functions.logistic_regression import log_likelihood\n",
    "log_likelihood.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`clf`データセットにロジスティック回帰を実行し、得られたモデルの対数尤度を算出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.158174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.460149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.377130</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.998298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x  y\n",
       "0  -0.158174  0\n",
       "1  -2.460149  0\n",
       "..       ... ..\n",
       "98 -1.377130  0\n",
       "99 -0.998298  0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = make_classification(n_features=1,\n",
    "                           n_informative=1,\n",
    "                           n_redundant=0,\n",
    "                           n_clusters_per_class=1,\n",
    "                           random_state=1234)\n",
    "clf = pd.DataFrame(dict(x=x.ravel(), y=y))\n",
    "print('clf')\n",
    "display(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-19.178769246079014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='none', solver='newton-cg').fit(\n",
    "    clf['x'].values.reshape((-1, 1)), clf['y'])\n",
    "proba = model.predict_proba(clf['x'].values.reshape((-1, 1)))\n",
    "log_likelihood = np.sum(np.log(proba[clf['y'] == 1, 1])) + np.sum(\n",
    "    np.log(proba[clf['y'] == 0, 0]))\n",
    "log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検定\n",
    "---\n",
    "パラメーターの標本分布を求めるのは困難なので、通常は中心極限定理によって近似し、自由度 $n-(k+1)$ の $t$ 分布を利用して $t$ 検定を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pythonでの検定の実行方法\n",
    "---\n",
    "`statsmodels.discrete.discrete_model.Logit`または`statsmodels.genmod.generalized_linear_model.GLM`を使用する。`GLM`でロジスティック回帰を適用するには`family`引数に`statsmodels.genmod.families.family.Binomial`を渡す。  \n",
    "いずれも切片項は自動的に追加されないので`statsmodels.tools.tools.add_constant`を使用して定数項を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass  sex\n",
       "0           0       3    1\n",
       "1           1       1    0\n",
       "..        ...     ...  ...\n",
       "889         1       1    1\n",
       "890         0       3    1\n",
       "\n",
       "[891 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic = sns.load_dataset('titanic')[['survived', 'pclass', 'sex']]\n",
    "titanic['sex'] = pd.Categorical(titanic['sex']).codes\n",
    "print('titanic')\n",
    "display(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Logit in module statsmodels.discrete.discrete_model:\n",
      "\n",
      "class Logit(BinaryModel)\n",
      " |  Logit(endog, exog, **kwargs)\n",
      " |  \n",
      " |  Logit Model\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      A 1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  endog : ndarray\n",
      " |      A reference to the endogenous response variable\n",
      " |  exog : ndarray\n",
      " |      A reference to the exogenous design.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Logit\n",
      " |      BinaryModel\n",
      " |      DiscreteModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  cdf(self, X)\n",
      " |      The logistic cumulative distribution function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          `X` is the linear predictor of the logit model.  See notes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      1/(1 + exp(-X))\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      In the logit model,\n",
      " |      \n",
      " |      .. math:: \\Lambda\\left(x^{\\prime}\\beta\\right)=\n",
      " |                \\text{Prob}\\left(Y=1|x\\right)=\n",
      " |                \\frac{e^{x^{\\prime}\\beta}}{1+e^{x^{\\prime}\\beta}}\n",
      " |  \n",
      " |  fit(self, start_params=None, method='newton', maxiter=35, full_output=1, disp=1, callback=None, **kwargs)\n",
      " |      Fit the model using maximum likelihood.\n",
      " |      \n",
      " |      The rest of the docstring is from\n",
      " |      statsmodels.base.model.LikelihoodModel.fit\n",
      " |      \n",
      " |      Fit method for likelihood based models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          The default is an array of zeros.\n",
      " |      method : str, optional\n",
      " |          The `method` determines which solver from `scipy.optimize`\n",
      " |          is used, and it can be chosen from among the following strings:\n",
      " |      \n",
      " |          - 'newton' for Newton-Raphson, 'nm' for Nelder-Mead\n",
      " |          - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
      " |          - 'lbfgs' for limited-memory BFGS with optional box constraints\n",
      " |          - 'powell' for modified Powell's method\n",
      " |          - 'cg' for conjugate gradient\n",
      " |          - 'ncg' for Newton-conjugate gradient\n",
      " |          - 'basinhopping' for global basin-hopping solver\n",
      " |          - 'minimize' for generic wrapper of scipy minimize (BFGS by default)\n",
      " |      \n",
      " |          The explicit arguments in `fit` are passed to the solver,\n",
      " |          with the exception of the basin-hopping solver. Each\n",
      " |          solver has several optional arguments that are not the same across\n",
      " |          solvers. See the notes section below (or scipy.optimize) for the\n",
      " |          available arguments and for the list of explicit arguments that the\n",
      " |          basin-hopping solver supports.\n",
      " |      maxiter : int, optional\n",
      " |          The maximum number of iterations to perform.\n",
      " |      full_output : bool, optional\n",
      " |          Set to True to have all available output in the Results object's\n",
      " |          mle_retvals attribute. The output is dependent on the solver.\n",
      " |          See LikelihoodModelResults notes section for more information.\n",
      " |      disp : bool, optional\n",
      " |          Set to True to print convergence messages.\n",
      " |      fargs : tuple, optional\n",
      " |          Extra arguments passed to the likelihood function, i.e.,\n",
      " |          loglike(x,*args)\n",
      " |      callback : callable callback(xk), optional\n",
      " |          Called after each iteration, as callback(xk), where xk is the\n",
      " |          current parameter vector.\n",
      " |      retall : bool, optional\n",
      " |          Set to True to return list of solutions at each iteration.\n",
      " |          Available in Results object's mle_retvals attribute.\n",
      " |      skip_hessian : bool, optional\n",
      " |          If False (default), then the negative inverse hessian is calculated\n",
      " |          after the optimization. If True, then the hessian will not be\n",
      " |          calculated. However, it will be available in methods that use the\n",
      " |          hessian in the optimization (currently only with `\"newton\"`).\n",
      " |      kwargs : keywords\n",
      " |          All kwargs are passed to the chosen solver with one exception. The\n",
      " |          following keyword controls what happens after the fit::\n",
      " |      \n",
      " |              warn_convergence : bool, optional\n",
      " |                  If True, checks the model for the converged flag. If the\n",
      " |                  converged flag is False, a ConvergenceWarning is issued.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The 'basinhopping' solver ignores `maxiter`, `retall`, `full_output`\n",
      " |      explicit arguments.\n",
      " |      \n",
      " |      Optional arguments for solvers (see returned Results.mle_settings)::\n",
      " |      \n",
      " |          'newton'\n",
      " |              tol : float\n",
      " |                  Relative error in params acceptable for convergence.\n",
      " |          'nm' -- Nelder Mead\n",
      " |              xtol : float\n",
      " |                  Relative error in params acceptable for convergence\n",
      " |              ftol : float\n",
      " |                  Relative error in loglike(params) acceptable for\n",
      " |                  convergence\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |          'bfgs'\n",
      " |              gtol : float\n",
      " |                  Stop when norm of gradient is less than gtol.\n",
      " |              norm : float\n",
      " |                  Order of norm (np.Inf is max, -np.Inf is min)\n",
      " |              epsilon\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Only relevant if LikelihoodModel.score is None.\n",
      " |          'lbfgs'\n",
      " |              m : int\n",
      " |                  This many terms are used for the Hessian approximation.\n",
      " |              factr : float\n",
      " |                  A stop condition that is a variant of relative error.\n",
      " |              pgtol : float\n",
      " |                  A stop condition that uses the projected gradient.\n",
      " |              epsilon\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Only relevant if LikelihoodModel.score is None.\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |              bounds : sequence\n",
      " |                  (min, max) pairs for each element in x,\n",
      " |                  defining the bounds on that parameter.\n",
      " |                  Use None for one of min or max when there is no bound\n",
      " |                  in that direction.\n",
      " |          'cg'\n",
      " |              gtol : float\n",
      " |                  Stop when norm of gradient is less than gtol.\n",
      " |              norm : float\n",
      " |                  Order of norm (np.Inf is max, -np.Inf is min)\n",
      " |              epsilon : float\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Can be scalar or vector.  Only relevant if\n",
      " |                  Likelihoodmodel.score is None.\n",
      " |          'ncg'\n",
      " |              fhess_p : callable f'(x,*args)\n",
      " |                  Function which computes the Hessian of f times an arbitrary\n",
      " |                  vector, p.  Should only be supplied if\n",
      " |                  LikelihoodModel.hessian is None.\n",
      " |              avextol : float\n",
      " |                  Stop when the average relative error in the minimizer\n",
      " |                  falls below this amount.\n",
      " |              epsilon : float or ndarray\n",
      " |                  If fhess is approximated, use this value for the step size.\n",
      " |                  Only relevant if Likelihoodmodel.hessian is None.\n",
      " |          'powell'\n",
      " |              xtol : float\n",
      " |                  Line-search error tolerance\n",
      " |              ftol : float\n",
      " |                  Relative error in loglike(params) for acceptable for\n",
      " |                  convergence.\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |              start_direc : ndarray\n",
      " |                  Initial direction set.\n",
      " |          'basinhopping'\n",
      " |              niter : int\n",
      " |                  The number of basin hopping iterations.\n",
      " |              niter_success : int\n",
      " |                  Stop the run if the global minimum candidate remains the\n",
      " |                  same for this number of iterations.\n",
      " |              T : float\n",
      " |                  The \"temperature\" parameter for the accept or reject\n",
      " |                  criterion. Higher \"temperatures\" mean that larger jumps\n",
      " |                  in function value will be accepted. For best results\n",
      " |                  `T` should be comparable to the separation (in function\n",
      " |                  value) between local minima.\n",
      " |              stepsize : float\n",
      " |                  Initial step size for use in the random displacement.\n",
      " |              interval : int\n",
      " |                  The interval for how often to update the `stepsize`.\n",
      " |              minimizer : dict\n",
      " |                  Extra keyword arguments to be passed to the minimizer\n",
      " |                  `scipy.optimize.minimize()`, for example 'method' - the\n",
      " |                  minimization method (e.g. 'L-BFGS-B'), or 'tol' - the\n",
      " |                  tolerance for termination. Other arguments are mapped from\n",
      " |                  explicit argument of `fit`:\n",
      " |                    - `args` <- `fargs`\n",
      " |                    - `jac` <- `score`\n",
      " |                    - `hess` <- `hess`\n",
      " |          'minimize'\n",
      " |              min_method : str, optional\n",
      " |                  Name of minimization method to use.\n",
      " |                  Any method specific arguments can be passed directly.\n",
      " |                  For a list of methods and their arguments, see\n",
      " |                  documentation of `scipy.optimize.minimize`.\n",
      " |                  If no method is specified, then BFGS is used.\n",
      " |  \n",
      " |  hessian(self, params)\n",
      " |      Logit model Hessian matrix of the log-likelihood\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hess : ndarray, (k_vars, k_vars)\n",
      " |          The Hessian, second derivative of loglikelihood function,\n",
      " |          evaluated at `params`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i}\\Lambda_{i}\\left(1-\\Lambda_{i}\\right)x_{i}x_{i}^{\\prime}\n",
      " |  \n",
      " |  loglike(self, params)\n",
      " |      Log-likelihood of logit model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the logit model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      loglike : float\n",
      " |          The log-likelihood function of the model evaluated at `params`.\n",
      " |          See notes.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |         \\ln L=\\sum_{i}\\ln\\Lambda\n",
      " |         \\left(q_{i}x_{i}^{\\prime}\\beta\\right)\n",
      " |      \n",
      " |      Where :math:`q=2y-1`. This simplification comes from the fact that the\n",
      " |      logistic distribution is symmetric.\n",
      " |  \n",
      " |  loglikeobs(self, params)\n",
      " |      Log-likelihood of logit model for each observation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the logit model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      loglike : ndarray\n",
      " |          The log likelihood for each observation of the model evaluated\n",
      " |          at `params`. See Notes\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |         \\ln L=\\sum_{i}\\ln\\Lambda\n",
      " |         \\left(q_{i}x_{i}^{\\prime}\\beta\\right)\n",
      " |      \n",
      " |      for observations :math:`i=1,...,n`\n",
      " |      \n",
      " |      where :math:`q=2y-1`. This simplification comes from the fact that the\n",
      " |      logistic distribution is symmetric.\n",
      " |  \n",
      " |  pdf(self, X)\n",
      " |      The logistic probability density function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          `X` is the linear predictor of the logit model.  See notes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pdf : ndarray\n",
      " |          The value of the Logit probability mass function, PMF, for each\n",
      " |          point of X. ``np.exp(-x)/(1+np.exp(-X))**2``\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      In the logit model,\n",
      " |      \n",
      " |      .. math:: \\lambda\\left(x^{\\prime}\\beta\\right)=\\frac{e^{-x^{\\prime}\\beta}}{\\left(1+e^{-x^{\\prime}\\beta}\\right)^{2}}\n",
      " |  \n",
      " |  score(self, params)\n",
      " |      Logit model score (gradient) vector of the log-likelihood\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray, 1-D\n",
      " |          The score vector of the model, i.e. the first derivative of the\n",
      " |          loglikelihood function, evaluated at `params`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\Lambda_{i}\\right)x_{i}\n",
      " |  \n",
      " |  score_obs(self, params)\n",
      " |      Logit model Jacobian of the log-likelihood for each observation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      jac : array_like\n",
      " |          The derivative of the loglikelihood for each observation evaluated\n",
      " |          at `params`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\Lambda_{i}\\right)x_{i}\n",
      " |      \n",
      " |      for observations :math:`i=1,...,n`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BinaryModel:\n",
      " |  \n",
      " |  __init__(self, endog, exog, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_regularized(self, start_params=None, method='l1', maxiter='defined_by_method', full_output=1, disp=1, callback=None, alpha=0, trim_mode='auto', auto_trim_tol=0.01, size_trim_tol=0.0001, qc_tol=0.03, **kwargs)\n",
      " |      Fit the model using a regularized maximum likelihood.\n",
      " |      \n",
      " |      The regularization method AND the solver used is determined by the\n",
      " |      argument method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          The default is an array of zeros.\n",
      " |      method : 'l1' or 'l1_cvxopt_cp'\n",
      " |          See notes for details.\n",
      " |      maxiter : {int, 'defined_by_method'}\n",
      " |          Maximum number of iterations to perform.\n",
      " |          If 'defined_by_method', then use method defaults (see notes).\n",
      " |      full_output : bool\n",
      " |          Set to True to have all available output in the Results object's\n",
      " |          mle_retvals attribute. The output is dependent on the solver.\n",
      " |          See LikelihoodModelResults notes section for more information.\n",
      " |      disp : bool\n",
      " |          Set to True to print convergence messages.\n",
      " |      fargs : tuple\n",
      " |          Extra arguments passed to the likelihood function, i.e.,\n",
      " |          loglike(x,*args).\n",
      " |      callback : callable callback(xk)\n",
      " |          Called after each iteration, as callback(xk), where xk is the\n",
      " |          current parameter vector.\n",
      " |      retall : bool\n",
      " |          Set to True to return list of solutions at each iteration.\n",
      " |          Available in Results object's mle_retvals attribute.\n",
      " |      alpha : non-negative scalar or numpy array (same size as parameters)\n",
      " |          The weight multiplying the l1 penalty term.\n",
      " |      trim_mode : 'auto, 'size', or 'off'\n",
      " |          If not 'off', trim (set to zero) parameters that would have been\n",
      " |          zero if the solver reached the theoretical minimum.\n",
      " |          If 'auto', trim params using the Theory above.\n",
      " |          If 'size', trim params if they have very small absolute value.\n",
      " |      size_trim_tol : float or 'auto' (default = 'auto')\n",
      " |          Tolerance used when trim_mode == 'size'.\n",
      " |      auto_trim_tol : float\n",
      " |          Tolerance used when trim_mode == 'auto'.\n",
      " |      qc_tol : float\n",
      " |          Print warning and do not allow auto trim when (ii) (above) is\n",
      " |          violated by this much.\n",
      " |      qc_verbose : bool\n",
      " |          If true, print out a full QC report upon failure.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments used when fitting the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Results\n",
      " |          A results instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Using 'l1_cvxopt_cp' requires the cvxopt module.\n",
      " |      \n",
      " |      Extra parameters are not penalized if alpha is given as a scalar.\n",
      " |      An example is the shape parameter in NegativeBinomial `nb1` and `nb2`.\n",
      " |      \n",
      " |      Optional arguments for the solvers (available in Results.mle_settings)::\n",
      " |      \n",
      " |          'l1'\n",
      " |              acc : float (default 1e-6)\n",
      " |                  Requested accuracy as used by slsqp\n",
      " |          'l1_cvxopt_cp'\n",
      " |              abstol : float\n",
      " |                  absolute accuracy (default: 1e-7).\n",
      " |              reltol : float\n",
      " |                  relative accuracy (default: 1e-6).\n",
      " |              feastol : float\n",
      " |                  tolerance for feasibility conditions (default: 1e-7).\n",
      " |              refinement : int\n",
      " |                  number of iterative refinement steps when solving KKT\n",
      " |                  equations (default: 1).\n",
      " |      \n",
      " |      Optimization methodology\n",
      " |      \n",
      " |      With :math:`L` the negative log likelihood, we solve the convex but\n",
      " |      non-smooth problem\n",
      " |      \n",
      " |      .. math:: \\min_\\beta L(\\beta) + \\sum_k\\alpha_k |\\beta_k|\n",
      " |      \n",
      " |      via the transformation to the smooth, convex, constrained problem\n",
      " |      in twice as many variables (adding the \"added variables\" :math:`u_k`)\n",
      " |      \n",
      " |      .. math:: \\min_{\\beta,u} L(\\beta) + \\sum_k\\alpha_k u_k,\n",
      " |      \n",
      " |      subject to\n",
      " |      \n",
      " |      .. math:: -u_k \\leq \\beta_k \\leq u_k.\n",
      " |      \n",
      " |      With :math:`\\partial_k L` the derivative of :math:`L` in the\n",
      " |      :math:`k^{th}` parameter direction, theory dictates that, at the\n",
      " |      minimum, exactly one of two conditions holds:\n",
      " |      \n",
      " |      (i) :math:`|\\partial_k L| = \\alpha_k`  and  :math:`\\beta_k \\neq 0`\n",
      " |      (ii) :math:`|\\partial_k L| \\leq \\alpha_k`  and  :math:`\\beta_k = 0`\n",
      " |  \n",
      " |  predict(self, params, exog=None, linear=False)\n",
      " |      Predict response variable of a model given exogenous variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Fitted parameters of the model.\n",
      " |      exog : array_like\n",
      " |          1d or 2d array of exogenous values.  If not supplied, the\n",
      " |          whole exog attribute of the model is used.\n",
      " |      linear : bool, optional\n",
      " |          If True, returns the linear predictor dot(exog,params).  Else,\n",
      " |          returns the value of the cdf at the linear predictor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array\n",
      " |          Fitted values at exog.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DiscreteModel:\n",
      " |  \n",
      " |  cov_params_func_l1(self, likelihood_model, xopt, retvals)\n",
      " |      Computes cov_params on a reduced parameter space\n",
      " |      corresponding to the nonzero parameters resulting from the\n",
      " |      l1 regularized fit.\n",
      " |      \n",
      " |      Returns a full cov_params matrix, with entries corresponding\n",
      " |      to zero'd values set to np.nan.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize is called by\n",
      " |      statsmodels.model.LikelihoodModel.__init__\n",
      " |      and should contain any preprocessing that needs to be done for a model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.Logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.464195\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>survived</td>     <th>  No. Observations:  </th>  <td>   891</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   888</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 24 May 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.3029</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>14:29:44</td>     <th>  Log-Likelihood:    </th> <td> -413.60</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -593.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.798e-79</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>    3.2946</td> <td>    0.297</td> <td>   11.077</td> <td> 0.000</td> <td>    2.712</td> <td>    3.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pclass</th> <td>   -0.9606</td> <td>    0.106</td> <td>   -9.057</td> <td> 0.000</td> <td>   -1.168</td> <td>   -0.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>    <td>   -2.6434</td> <td>    0.184</td> <td>  -14.380</td> <td> 0.000</td> <td>   -3.004</td> <td>   -2.283</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               survived   No. Observations:                  891\n",
       "Model:                          Logit   Df Residuals:                      888\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 24 May 2020   Pseudo R-squ.:                  0.3029\n",
       "Time:                        14:29:44   Log-Likelihood:                -413.60\n",
       "converged:                       True   LL-Null:                       -593.33\n",
       "Covariance Type:            nonrobust   LLR p-value:                 8.798e-79\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.2946      0.297     11.077      0.000       2.712       3.878\n",
       "pclass        -0.9606      0.106     -9.057      0.000      -1.168      -0.753\n",
       "sex           -2.6434      0.184    -14.380      0.000      -3.004      -2.283\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sm.add_constant(titanic[['pclass', 'sex']])\n",
    "model = sm.Logit(titanic['survived'], x).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GLM in module statsmodels.genmod.generalized_linear_model:\n",
      "\n",
      "class GLM(statsmodels.base.model.LikelihoodModel)\n",
      " |  GLM(endog, exog, family=None, offset=None, exposure=None, freq_weights=None, var_weights=None, missing='none', **kwargs)\n",
      " |  \n",
      " |  Generalized Linear Models\n",
      " |  \n",
      " |  GLM inherits from statsmodels.base.model.LikelihoodModel\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      1d array of endogenous response variable.  This array can be 1d or 2d.\n",
      " |      Binomial family models accept a 2d array with two columns. If\n",
      " |      supplied, each observation is expected to be [success, failure].\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user (models specified using a formula\n",
      " |      include an intercept by default). See `statsmodels.tools.add_constant`.\n",
      " |  family : family class instance\n",
      " |      The default is Gaussian.  To specify the binomial distribution\n",
      " |      family = sm.family.Binomial()\n",
      " |      Each family can take a link instance as an argument.  See\n",
      " |      statsmodels.family.family for more information.\n",
      " |  offset : array_like or None\n",
      " |      An offset to be included in the model.  If provided, must be\n",
      " |      an array whose length is the number of rows in exog.\n",
      " |  exposure : array_like or None\n",
      " |      Log(exposure) will be added to the linear prediction in the model.\n",
      " |      Exposure is only valid if the log link is used. If provided, it must be\n",
      " |      an array with the same length as endog.\n",
      " |  freq_weights : array_like\n",
      " |      1d array of frequency weights. The default is None. If None is selected\n",
      " |      or a blank value, then the algorithm will replace with an array of 1's\n",
      " |      with length equal to the endog.\n",
      " |      WARNING: Using weights is not verified yet for all possible options\n",
      " |      and results, see Notes.\n",
      " |  var_weights : array_like\n",
      " |      1d array of variance (analytic) weights. The default is None. If None\n",
      " |      is selected or a blank value, then the algorithm will replace with an\n",
      " |      array of 1's with length equal to the endog.\n",
      " |      WARNING: Using weights is not verified yet for all possible options\n",
      " |      and results, see Notes.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  df_model : float\n",
      " |      Model degrees of freedom is equal to p - 1, where p is the number\n",
      " |      of regressors.  Note that the intercept is not reported as a\n",
      " |      degree of freedom.\n",
      " |  df_resid : float\n",
      " |      Residual degrees of freedom is equal to the number of observation n\n",
      " |      minus the number of regressors p.\n",
      " |  endog : ndarray\n",
      " |      See Notes.  Note that `endog` is a reference to the data so that if\n",
      " |      data is already an array and it is changed, then `endog` changes\n",
      " |      as well.\n",
      " |  exposure : array_like\n",
      " |      Include ln(exposure) in model with coefficient constrained to 1. Can\n",
      " |      only be used if the link is the logarithm function.\n",
      " |  exog : ndarray\n",
      " |      See Notes.  Note that `exog` is a reference to the data so that if\n",
      " |      data is already an array and it is changed, then `exog` changes\n",
      " |      as well.\n",
      " |  freq_weights : ndarray\n",
      " |      See Notes. Note that `freq_weights` is a reference to the data so that\n",
      " |      if data is already an array and it is changed, then `freq_weights`\n",
      " |      changes as well.\n",
      " |  var_weights : ndarray\n",
      " |      See Notes. Note that `var_weights` is a reference to the data so that\n",
      " |      if data is already an array and it is changed, then `var_weights`\n",
      " |      changes as well.\n",
      " |  iteration : int\n",
      " |      The number of iterations that fit has run.  Initialized at 0.\n",
      " |  family : family class instance\n",
      " |      The distribution family of the model. Can be any family in\n",
      " |      statsmodels.families.  Default is Gaussian.\n",
      " |  mu : ndarray\n",
      " |      The mean response of the transformed variable.  `mu` is the value of\n",
      " |      the inverse of the link function at lin_pred, where lin_pred is the\n",
      " |      linear predicted value of the WLS fit of the transformed variable.\n",
      " |      `mu` is only available after fit is called.  See\n",
      " |      statsmodels.families.family.fitted of the distribution family for more\n",
      " |      information.\n",
      " |  n_trials : ndarray\n",
      " |      See Notes. Note that `n_trials` is a reference to the data so that if\n",
      " |      data is already an array and it is changed, then `n_trials` changes\n",
      " |      as well. `n_trials` is the number of binomial trials and only available\n",
      " |      with that distribution. See statsmodels.families.Binomial for more\n",
      " |      information.\n",
      " |  normalized_cov_params : ndarray\n",
      " |      The p x p normalized covariance of the design / exogenous data.\n",
      " |      This is approximately equal to (X.T X)^(-1)\n",
      " |  offset : array_like\n",
      " |      Include offset in model with coefficient constrained to 1.\n",
      " |  scale : float\n",
      " |      The estimate of the scale / dispersion of the model fit.  Only\n",
      " |      available after fit is called.  See GLM.fit and GLM.estimate_scale\n",
      " |      for more information.\n",
      " |  scaletype : str\n",
      " |      The scaling used for fitting the model.  This is only available after\n",
      " |      fit is called.  The default is None.  See GLM.fit for more information.\n",
      " |  weights : ndarray\n",
      " |      The value of the weights after the last iteration of fit.  Only\n",
      " |      available after fit is called.  See statsmodels.families.family for\n",
      " |      the specific distribution weighting functions.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import statsmodels.api as sm\n",
      " |  >>> data = sm.datasets.scotland.load(as_pandas=False)\n",
      " |  >>> data.exog = sm.add_constant(data.exog)\n",
      " |  \n",
      " |  Instantiate a gamma family model with the default link function.\n",
      " |  \n",
      " |  >>> gamma_model = sm.GLM(data.endog, data.exog,\n",
      " |  ...                      family=sm.families.Gamma())\n",
      " |  \n",
      " |  >>> gamma_results = gamma_model.fit()\n",
      " |  >>> gamma_results.params\n",
      " |  array([-0.01776527,  0.00004962,  0.00203442, -0.00007181,  0.00011185,\n",
      " |         -0.00000015, -0.00051868, -0.00000243])\n",
      " |  >>> gamma_results.scale\n",
      " |  0.0035842831734919055\n",
      " |  >>> gamma_results.deviance\n",
      " |  0.087388516416999198\n",
      " |  >>> gamma_results.pearson_chi2\n",
      " |  0.086022796163805704\n",
      " |  >>> gamma_results.llf\n",
      " |  -83.017202161073527\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  statsmodels.genmod.families.family.Family\n",
      " |  :ref:`families`\n",
      " |  :ref:`links`\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Only the following combinations make sense for family and link:\n",
      " |  \n",
      " |   ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n",
      " |   Family        ident log logit probit cloglog pow opow nbinom loglog logc\n",
      " |   ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n",
      " |   Gaussian      x     x   x     x      x       x   x     x      x\n",
      " |   inv Gaussian  x     x                        x\n",
      " |   binomial      x     x   x     x      x       x   x           x      x\n",
      " |   Poisson       x     x                        x\n",
      " |   neg binomial  x     x                        x        x\n",
      " |   gamma         x     x                        x\n",
      " |   Tweedie       x     x                        x\n",
      " |   ============= ===== === ===== ====== ======= === ==== ====== ====== ====\n",
      " |  \n",
      " |  Not all of these link functions are currently available.\n",
      " |  \n",
      " |  Endog and exog are references so that if the data they refer to are already\n",
      " |  arrays and these arrays are changed, endog and exog will change.\n",
      " |  \n",
      " |  statsmodels supports two separate definitions of weights: frequency weights\n",
      " |  and variance weights.\n",
      " |  \n",
      " |  Frequency weights produce the same results as repeating observations by the\n",
      " |  frequencies (if those are integers). Frequency weights will keep the number\n",
      " |  of observations consistent, but the degrees of freedom will change to\n",
      " |  reflect the new weights.\n",
      " |  \n",
      " |  Variance weights (referred to in other packages as analytic weights) are\n",
      " |  used when ``endog`` represents an an average or mean. This relies on the\n",
      " |  assumption that that the inverse variance scales proportionally to the\n",
      " |  weight--an observation that is deemed more credible should have less\n",
      " |  variance and therefore have more weight. For the ``Poisson`` family--which\n",
      " |  assumes that occurrences scale proportionally with time--a natural practice\n",
      " |  would be to use the amount of time as the variance weight and set ``endog``\n",
      " |  to be a rate (occurrences per period of time). Similarly, using a\n",
      " |  compound Poisson family, namely ``Tweedie``, makes a similar assumption\n",
      " |  about the rate (or frequency) of occurrences having variance proportional to\n",
      " |  time.\n",
      " |  \n",
      " |  Both frequency and variance weights are verified for all basic results with\n",
      " |  nonrobust or heteroscedasticity robust ``cov_type``. Other robust\n",
      " |  covariance types have not yet been verified, and at least the small sample\n",
      " |  correction is currently not based on the correct total frequency count.\n",
      " |  \n",
      " |  Currently, all residuals are not weighted by frequency, although they may\n",
      " |  incorporate ``n_trials`` for ``Binomial`` and ``var_weights``\n",
      " |  \n",
      " |  +---------------+----------------------------------+\n",
      " |  | Residual Type | Applicable weights               |\n",
      " |  +===============+==================================+\n",
      " |  | Anscombe      | ``var_weights``                  |\n",
      " |  +---------------+----------------------------------+\n",
      " |  | Deviance      | ``var_weights``                  |\n",
      " |  +---------------+----------------------------------+\n",
      " |  | Pearson       | ``var_weights`` and ``n_trials`` |\n",
      " |  +---------------+----------------------------------+\n",
      " |  | Reponse       | ``n_trials``                     |\n",
      " |  +---------------+----------------------------------+\n",
      " |  | Working       | ``n_trials``                     |\n",
      " |  +---------------+----------------------------------+\n",
      " |  \n",
      " |  WARNING: Loglikelihood and deviance are not valid in models where\n",
      " |  scale is equal to 1 (i.e., ``Binomial``, ``NegativeBinomial``, and\n",
      " |  ``Poisson``). If variance weights are specified, then results such as\n",
      " |  ``loglike`` and ``deviance`` are based on a quasi-likelihood\n",
      " |  interpretation. The loglikelihood is not correctly specified in this case,\n",
      " |  and statistics based on it, such AIC or likelihood ratio tests, are not\n",
      " |  appropriate.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GLM\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog, family=None, offset=None, exposure=None, freq_weights=None, var_weights=None, missing='none', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  estimate_scale(self, mu)\n",
      " |      Estimate the dispersion/scale.\n",
      " |      \n",
      " |      Type of scale can be chose in the fit method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mu : ndarray\n",
      " |          mu is the mean response estimate\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Estimate of scale\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default scale for Binomial, Poisson and Negative Binomial\n",
      " |      families is 1.  The default for the other families is Pearson's\n",
      " |      Chi-Square estimate.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      statsmodels.genmod.generalized_linear_model.GLM.fit\n",
      " |  \n",
      " |  estimate_tweedie_power(self, mu, method='brentq', low=1.01, high=5.0)\n",
      " |      Tweedie specific function to estimate scale and the variance parameter.\n",
      " |      The variance parameter is also referred to as p, xi, or shape.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mu : array_like\n",
      " |          Fitted mean response variable\n",
      " |      method : str, defaults to 'brentq'\n",
      " |          Scipy optimizer used to solve the Pearson equation. Only brentq\n",
      " |          currently supported.\n",
      " |      low : float, optional\n",
      " |          Low end of the bracketing interval [a,b] to be used in the search\n",
      " |          for the power. Defaults to 1.01.\n",
      " |      high : float, optional\n",
      " |          High end of the bracketing interval [a,b] to be used in the search\n",
      " |          for the power. Defaults to 5.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      power : float\n",
      " |          The estimated shape or power.\n",
      " |  \n",
      " |  fit(self, start_params=None, maxiter=100, method='IRLS', tol=1e-08, scale=None, cov_type='nonrobust', cov_kwds=None, use_t=None, full_output=True, disp=False, max_start_irls=3, **kwargs)\n",
      " |      Fits a generalized linear model for a given family.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          The default is family-specific and is given by the\n",
      " |          ``family.starting_mu(endog)``. If start_params is given then the\n",
      " |          initial mean will be calculated as ``np.dot(exog, start_params)``.\n",
      " |      maxiter : int, optional\n",
      " |          Default is 100.\n",
      " |      method : str\n",
      " |          Default is 'IRLS' for iteratively reweighted least squares.\n",
      " |          Otherwise gradient optimization is used.\n",
      " |      tol : float\n",
      " |          Convergence tolerance.  Default is 1e-8.\n",
      " |      scale : str or float, optional\n",
      " |          `scale` can be 'X2', 'dev', or a float\n",
      " |          The default value is None, which uses `X2` for Gamma, Gaussian,\n",
      " |          and Inverse Gaussian.\n",
      " |          `X2` is Pearson's chi-square divided by `df_resid`.\n",
      " |          The default is 1 for the Binomial and Poisson families.\n",
      " |          `dev` is the deviance divided by df_resid\n",
      " |      cov_type : str\n",
      " |          The type of parameter estimate covariance matrix to compute.\n",
      " |      cov_kwds : dict-like\n",
      " |          Extra arguments for calculating the covariance of the parameter\n",
      " |          estimates.\n",
      " |      use_t : bool\n",
      " |          If True, the Student t-distribution is used for inference.\n",
      " |      full_output : bool, optional\n",
      " |          Set to True to have all available output in the Results object's\n",
      " |          mle_retvals attribute. The output is dependent on the solver.\n",
      " |          See LikelihoodModelResults notes section for more information.\n",
      " |          Not used if methhod is IRLS.\n",
      " |      disp : bool, optional\n",
      " |          Set to True to print convergence messages.  Not used if method is\n",
      " |          IRLS.\n",
      " |      max_start_irls : int\n",
      " |          The number of IRLS iterations used to obtain starting\n",
      " |          values for gradient optimization.  Only relevant if\n",
      " |          `method` is set to something other than 'IRLS'.\n",
      " |      atol : float, optional\n",
      " |          (available with IRLS fits) The absolute tolerance criterion that\n",
      " |          must be satisfied. Defaults to ``tol``. Convergence is attained\n",
      " |          when: :math:`rtol * prior + atol > abs(current - prior)`\n",
      " |      rtol : float, optional\n",
      " |          (available with IRLS fits) The relative tolerance criterion that\n",
      " |          must be satisfied. Defaults to 0 which means ``rtol`` is not used.\n",
      " |          Convergence is attained when:\n",
      " |          :math:`rtol * prior + atol > abs(current - prior)`\n",
      " |      tol_criterion : str, optional\n",
      " |          (available with IRLS fits) Defaults to ``'deviance'``. Can\n",
      " |          optionally be ``'params'``.\n",
      " |      wls_method : str, optional\n",
      " |          (available with IRLS fits) options are 'lstsq', 'pinv' and 'qr'\n",
      " |          specifies which linear algebra function to use for the irls\n",
      " |          optimization. Default is `lstsq` which uses the same underlying\n",
      " |          svd based approach as 'pinv', but is faster during iterations.\n",
      " |          'lstsq' and 'pinv' regularize the estimate in singular and\n",
      " |          near-singular cases by truncating small singular values based\n",
      " |          on `rcond` of the respective numpy.linalg function. 'qr' is\n",
      " |          only valid for cases that are not singular nor near-singular.\n",
      " |      optim_hessian : {'eim', 'oim'}, optional\n",
      " |          (available with scipy optimizer fits) When 'oim'--the default--the\n",
      " |          observed Hessian is used in fitting. 'eim' is the expected Hessian.\n",
      " |          This may provide more stable fits, but adds assumption that the\n",
      " |          Hessian is correctly specified.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If method is 'IRLS', then an additional keyword 'attach_wls' is\n",
      " |      available. This is currently for internal use only and might change\n",
      " |      in future versions. If attach_wls' is true, then the final WLS\n",
      " |      instance of the IRLS iteration is attached to the results instance\n",
      " |      as `results_wls` attribute.\n",
      " |  \n",
      " |  fit_constrained(self, constraints, start_params=None, **fit_kwds)\n",
      " |      fit the model subject to linear equality constraints\n",
      " |      \n",
      " |      The constraints are of the form   `R params = q`\n",
      " |      where R is the constraint_matrix and q is the vector of\n",
      " |      constraint_values.\n",
      " |      \n",
      " |      The estimation creates a new model with transformed design matrix,\n",
      " |      exog, and converts the results back to the original parameterization.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      constraints : formula expression or tuple\n",
      " |          If it is a tuple, then the constraint needs to be given by two\n",
      " |          arrays (constraint_matrix, constraint_value), i.e. (R, q).\n",
      " |          Otherwise, the constraints can be given as strings or list of\n",
      " |          strings.\n",
      " |          see t_test for details\n",
      " |      start_params : None or array_like\n",
      " |          starting values for the optimization. `start_params` needs to be\n",
      " |          given in the original parameter space and are internally\n",
      " |          transformed.\n",
      " |      **fit_kwds : keyword arguments\n",
      " |          fit_kwds are used in the optimization of the transformed model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results : Results instance\n",
      " |  \n",
      " |  fit_regularized(self, method='elastic_net', alpha=0.0, start_params=None, refit=False, opt_method='bfgs', **kwargs)\n",
      " |      Return a regularized fit to a linear regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : {'elastic_net'}\n",
      " |          Only the `elastic_net` approach is currently implemented.\n",
      " |      alpha : scalar or array_like\n",
      " |          The penalty weight.  If a scalar, the same penalty weight\n",
      " |          applies to all variables in the model.  If a vector, it\n",
      " |          must have the same length as `params`, and contains a\n",
      " |          penalty weight for each coefficient.\n",
      " |      start_params : array_like\n",
      " |          Starting values for `params`.\n",
      " |      refit : bool\n",
      " |          If True, the model is refit using only the variables that\n",
      " |          have non-zero coefficients in the regularized fit.  The\n",
      " |          refitted model is not regularized.\n",
      " |      opt_method : string\n",
      " |          The method used for numerical optimization.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments used when fitting the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      GLMResults\n",
      " |          An array or a GLMResults object, same type returned by `fit`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The penalty is the ``elastic net`` penalty, which is a\n",
      " |      combination of L1 and L2 penalties.\n",
      " |      \n",
      " |      The function that is minimized is:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          -loglike/n + alpha*((1-L1\\_wt)*|params|_2^2/2 + L1\\_wt*|params|_1)\n",
      " |      \n",
      " |      where :math:`|*|_1` and :math:`|*|_2` are the L1 and L2 norms.\n",
      " |      \n",
      " |      Post-estimation results are based on the same data used to\n",
      " |      select variables, hence may be subject to overfitting biases.\n",
      " |      \n",
      " |      The elastic_net method uses the following keyword arguments:\n",
      " |      \n",
      " |      maxiter : int\n",
      " |          Maximum number of iterations\n",
      " |      L1_wt  : float\n",
      " |          Must be in [0, 1].  The L1 penalty has weight L1_wt and the\n",
      " |          L2 penalty has weight 1 - L1_wt.\n",
      " |      cnvrg_tol : float\n",
      " |          Convergence threshold for line searches\n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |  \n",
      " |  get_distribution(self, params, scale=1, exog=None, exposure=None, offset=None)\n",
      " |      Return a random number generator for the predictive distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The model parameters.\n",
      " |      scale : scalar\n",
      " |          The scale parameter.\n",
      " |      exog : array_like\n",
      " |          The predictor variable matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      gen\n",
      " |          Frozen random number generator object.  Use the ``rvs`` method to\n",
      " |          generate random values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to the behavior of ``scipy.stats.distributions objects``, the\n",
      " |      returned random number generator must be called with ``gen.rvs(n)``\n",
      " |      where ``n`` is the number of observations in the data set used\n",
      " |      to fit the model.  If any other value is used for ``n``, misleading\n",
      " |      results will be produced.\n",
      " |  \n",
      " |  hessian(self, params, scale=None, observed=None)\n",
      " |      Hessian, second derivative of loglikelihood function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          parameter at which Hessian is evaluated\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is returned (default).\n",
      " |          If false then the expected information matrix is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hessian : ndarray\n",
      " |          Hessian, i.e. observed information, or expected information matrix.\n",
      " |  \n",
      " |  hessian_factor(self, params, scale=None, observed=True)\n",
      " |      Weights for calculating Hessian\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          parameter at which Hessian is evaluated\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is returned. If false then the\n",
      " |          expected information matrix is returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hessian_factor : ndarray, 1d\n",
      " |          A 1d weight vector used in the calculation of the Hessian.\n",
      " |          The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`\n",
      " |  \n",
      " |  information(self, params, scale=None)\n",
      " |      Fisher information matrix.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize a generalized linear model.\n",
      " |  \n",
      " |  loglike(self, params, scale=None)\n",
      " |      Evaluate the log-likelihood for a generalized linear model.\n",
      " |  \n",
      " |  loglike_mu(self, mu, scale=1.0)\n",
      " |      Evaluate the log-likelihood for a generalized linear model.\n",
      " |  \n",
      " |  predict(self, params, exog=None, exposure=None, offset=None, linear=False)\n",
      " |      Return predicted values for a design matrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Parameters / coefficients of a GLM.\n",
      " |      exog : array_like, optional\n",
      " |          Design / exogenous data. Is exog is None, model exog is used.\n",
      " |      exposure : array_like, optional\n",
      " |          Exposure time values, only can be used with the log link\n",
      " |          function.  See notes for details.\n",
      " |      offset : array_like, optional\n",
      " |          Offset values.  See notes for details.\n",
      " |      linear : bool\n",
      " |          If True, returns the linear predicted values.  If False,\n",
      " |          returns the value of the inverse of the model's link function at\n",
      " |          the linear predicted values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      An array of fitted values\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Any `exposure` and `offset` provided here take precedence over\n",
      " |      the `exposure` and `offset` used in the model fit.  If `exog`\n",
      " |      is passed as an argument here, then any `exposure` and\n",
      " |      `offset` values in the fit will be ignored.\n",
      " |      \n",
      " |      Exposure values must be strictly positive.\n",
      " |  \n",
      " |  score(self, params, scale=None)\n",
      " |      score, first derivative of the loglikelihood function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          parameter at which score is evaluated\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray_1d\n",
      " |          The first derivative of the loglikelihood function calculated as\n",
      " |          the sum of `score_obs`\n",
      " |  \n",
      " |  score_factor(self, params, scale=None)\n",
      " |      weights for score for each observation\n",
      " |      \n",
      " |      This can be considered as score residuals.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          parameter at which score is evaluated\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score_factor : ndarray_1d\n",
      " |          A 1d weight vector used in the calculation of the score_obs.\n",
      " |          The score_obs are obtained by `score_factor[:, None] * exog`\n",
      " |  \n",
      " |  score_obs(self, params, scale=None)\n",
      " |      score first derivative of the loglikelihood for each observation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          parameter at which score is evaluated\n",
      " |      scale : None or float\n",
      " |          If scale is None, then the default scale will be calculated.\n",
      " |          Default scale is defined by `self.scaletype` and set in fit.\n",
      " |          If scale is not None, then it is used as a fixed scale.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score_obs : ndarray, 2d\n",
      " |          The first derivative of the loglikelihood function evaluated at\n",
      " |          params for each observation.\n",
      " |  \n",
      " |  score_test(self, params_constrained, k_constraints=None, exog_extra=None, observed=True)\n",
      " |      score test for restrictions or for omitted variables\n",
      " |      \n",
      " |      The covariance matrix for the score is based on the Hessian, i.e.\n",
      " |      observed information matrix or optionally on the expected information\n",
      " |      matrix..\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params_constrained : array_like\n",
      " |          estimated parameter of the restricted model. This can be the\n",
      " |          parameter estimate for the current when testing for omitted\n",
      " |          variables.\n",
      " |      k_constraints : int or None\n",
      " |          Number of constraints that were used in the estimation of params\n",
      " |          restricted relative to the number of exog in the model.\n",
      " |          This must be provided if no exog_extra are given. If exog_extra is\n",
      " |          not None, then k_constraints is assumed to be zero if it is None.\n",
      " |      exog_extra : None or array_like\n",
      " |          Explanatory variables that are jointly tested for inclusion in the\n",
      " |          model, i.e. omitted variables.\n",
      " |      observed : bool\n",
      " |          If True, then the observed Hessian is used in calculating the\n",
      " |          covariance matrix of the score. If false then the expected\n",
      " |          information matrix is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      chi2_stat : float\n",
      " |          chisquare statistic for the score test\n",
      " |      p-value : float\n",
      " |          P-value of the score test based on the chisquare distribution.\n",
      " |      df : int\n",
      " |          Degrees of freedom used in the p-value calculation. This is equal\n",
      " |          to the number of constraints.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      not yet verified for case with scale not equal to 1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>survived</td>     <th>  No. Observations:  </th>  <td>   891</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>   888</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -413.60</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 24 May 2020</td> <th>  Deviance:          </th> <td>  827.20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>14:29:44</td>     <th>  Pearson chi2:      </th>  <td>  911.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>    3.2946</td> <td>    0.297</td> <td>   11.077</td> <td> 0.000</td> <td>    2.712</td> <td>    3.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pclass</th> <td>   -0.9606</td> <td>    0.106</td> <td>   -9.057</td> <td> 0.000</td> <td>   -1.168</td> <td>   -0.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>    <td>   -2.6434</td> <td>    0.184</td> <td>  -14.380</td> <td> 0.000</td> <td>   -3.004</td> <td>   -2.283</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:               survived   No. Observations:                  891\n",
       "Model:                            GLM   Df Residuals:                      888\n",
       "Model Family:                Binomial   Df Model:                            2\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -413.60\n",
       "Date:                Sun, 24 May 2020   Deviance:                       827.20\n",
       "Time:                        14:29:44   Pearson chi2:                     911.\n",
       "No. Iterations:                     5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.2946      0.297     11.077      0.000       2.712       3.878\n",
       "pclass        -0.9606      0.106     -9.057      0.000      -1.168      -0.753\n",
       "sex           -2.6434      0.184    -14.380      0.000      -3.004      -2.283\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.GLM(titanic['survived'], x, family=sm.families.Binomial()).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多項ロジスティック回帰\n",
    "---\n",
    "目的変数のクラス数 (水準) が 3 以上の場合のロジスティック回帰。\n",
    "\n",
    "$k$ 個の特徴を持つサンプル $\\boldsymbol{x} =( x_{1} ,x_{2} ,\\cdots ,x_{k})^{T}$ に対して $c$ 個のクラスを持つ目的変数を予測する場合、サイズ $c\\times k$ の行列 $\\boldsymbol{W}$ を用いて、要素数 $c$ のベクトル $\\boldsymbol{z} =\\boldsymbol{Wx}$ を作成する。その $z$ に対してソフトマックス関数を適用して確率を算出する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数\n",
    "---\n",
    "クラス数 $c$ 中の $i$ 番目のクラスに対応する確率を以下のように表すのがソフトマックス関数。\n",
    "\n",
    "$\n",
    "\\displaystyle \\begin{aligned}\n",
    "    \\hat{y}_{i} & =\\frac\n",
    "        {e^{z_{i}}}\n",
    "        {\\displaystyle \\sum ^{c}_{j=1} e^{z_{j}}}\\\\\n",
    "     & \\\\\n",
    "     & =\\frac\n",
    "         {e^{z_{i}}}\n",
    "         {e^{z_{1}} +e^{z_{2}} +\\cdots +e^{z_{c}}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "クラス数を $2$ とした場合、\n",
    "\n",
    "$\n",
    "\\displaystyle \\begin{aligned}\n",
    "    \\hat{y}_{1} & =\\frac\n",
    "        {e^{z_{1}}}\n",
    "        {e^{z_{1}} +e^{z_{2}}}\n",
    "    \\\\\n",
    "     & \\\\\n",
    "     & =\\frac\n",
    "         {1}\n",
    "         {1+e^{z_{2} -z_{1}}}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "となり、ロジスティック関数 $\\displaystyle f( x) =\\frac{1}{1+e^{-x}}$ と一致する。つまり、ソフトマックス関数はロジスティック関数の多クラスへの拡張 (ロジスティック関数はクラス数 $2$ の場合のソフトマックス関数の特殊形) である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`iris`データセットに対して、各クラスを予測するための重み行列`w`(値は乱数でよい) を作成し、ソフトマックス関数を適用して予測値を算出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "print('iris')\n",
    "display(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000190</td>\n",
       "      <td>2.301758e-09</td>\n",
       "      <td>0.999810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000420</td>\n",
       "      <td>8.688983e-09</td>\n",
       "      <td>0.999580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.547846e-11</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>7.269393e-11</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0             1         2\n",
       "0    0.000190  2.301758e-09  0.999810\n",
       "1    0.000420  8.688983e-09  0.999580\n",
       "..        ...           ...       ...\n",
       "148  0.000002  1.547846e-11  0.999998\n",
       "149  0.000021  7.269393e-11  0.999979\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "w = np.random.normal(size=(iris.iloc[:, :-1].shape[1],\n",
    "                           iris['species'].nunique()))\n",
    "z = iris.iloc[:, :-1].values.dot(w)\n",
    "e = np.exp(z)\n",
    "pd.DataFrame(e / e.sum(axis=1).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推薦図書\n",
    "---\n",
    "- [Python 機械学習プログラミング 達人データサイエンティストによる理論と実践](https://www.amazon.co.jp/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E9%81%94%E4%BA%BA%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E3%81%AB%E3%82%88%E3%82%8B%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%B7%B5-impress-gear/dp/4295003379/)\n",
    "- [機械学習のエッセンス -実装しながら学ぶPython,数学,アルゴリズム-](https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E3%82%A8%E3%83%83%E3%82%BB%E3%83%B3%E3%82%B9-%E5%AE%9F%E8%A3%85%E3%81%97%E3%81%AA%E3%81%8C%E3%82%89%E5%AD%A6%E3%81%B6Python-%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0-Machine-Learning/dp/4797393963/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
