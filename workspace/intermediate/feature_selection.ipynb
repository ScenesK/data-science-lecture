{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_breast_cancer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from IPython.display import display\n",
    "pd.set_option('max_rows', 5)\n",
    "pd.set_option('max_columns', 9)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴選択\n",
    "---\n",
    "データセットに存在する特徴から、モデル構築に使用する特徴を一部に絞り込むこと。  \n",
    "利用する特徴を減らすことで、ストレージ・メモリの使用量を減らしたり、モデルを早く動かせる。また、データ理解という観点からは同程度の精度ならばより少ない特徴で記述したほうが理解しやすくなる。\n",
    "\n",
    "正則化が適用できないモデルでは過学習の防止に役立つが、正則化が適用できるモデルでは**過学習防止には正則化**を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価指標に基づくもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップワイズ法\n",
    "---\n",
    "評価指標が改善する特徴を追加・削除していく。\n",
    "\n",
    "評価指標には AIC (Akaike's Information Criterion, 赤池情報量規準) や BIC (Bayesian Information Criterion, ベイズ情報量規準) を用いることが多い。 (モデルの複雑さと精度のバランスをとるため)  \n",
    "絞り込みたい特徴の数が決まっている場合には他の一般的な評価指標 (accuracy や MSE など) も利用される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward stepwise selection\n",
    "---\n",
    "最も評価指標を改善する特徴を 1 つずつ追加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pythonによるforward stepwise selectionの実行方法\n",
    "---\n",
    "`mlxtend.feature_selection.SequentialFeatureSelector`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS  ...  PTRATIO       B  LSTAT  target\n",
       "0    0.00632  18.0   2.31   0.0  ...     15.3  396.90   4.98    24.0\n",
       "1    0.02731   0.0   7.07   0.0  ...     17.8  396.90   9.14    21.6\n",
       "..       ...   ...    ...   ...  ...      ...     ...    ...     ...\n",
       "504  0.10959   0.0  11.93   0.0  ...     21.0  393.45   6.48    22.0\n",
       "505  0.04741   0.0  11.93   0.0  ...     21.0  396.90   7.88    11.9\n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = load_boston()\n",
    "boston = pd.DataFrame(np.column_stack([loader.data, loader.target]),\n",
    "                      columns=list(loader.feature_names) + ['target'])\n",
    "print('boston')\n",
    "display(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = boston.iloc[:, :-1]\n",
    "y = boston.iloc[:, -1]\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SequentialFeatureSelector in module mlxtend.feature_selection.sequential_feature_selector:\n",
      "\n",
      "class SequentialFeatureSelector(mlxtend.utils.base_compostion._BaseXComposition, sklearn.base.MetaEstimatorMixin)\n",
      " |  SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True, fixed_features=None)\n",
      " |  \n",
      " |  Sequential Feature Selection for Classification and Regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : scikit-learn classifier or regressor\n",
      " |  k_features : int or tuple or str (default: 1)\n",
      " |      Number of features to select,\n",
      " |      where k_features < the full feature set.\n",
      " |      New in 0.4.2: A tuple containing a min and max value can be provided,\n",
      " |          and the SFS will consider return any feature combination between\n",
      " |          min and max that scored highest in cross-validtion. For example,\n",
      " |          the tuple (1, 4) will return any combination from\n",
      " |          1 up to 4 features instead of a fixed number of features k.\n",
      " |      New in 0.8.0: A string argument \"best\" or \"parsimonious\".\n",
      " |          If \"best\" is provided, the feature selector will return the\n",
      " |          feature subset with the best cross-validation performance.\n",
      " |          If \"parsimonious\" is provided as an argument, the smallest\n",
      " |          feature subset that is within one standard error of the\n",
      " |          cross-validation performance will be selected.\n",
      " |  forward : bool (default: True)\n",
      " |      Forward selection if True,\n",
      " |      backward selection otherwise\n",
      " |  floating : bool (default: False)\n",
      " |      Adds a conditional exclusion/inclusion if True.\n",
      " |  verbose : int (default: 0), level of verbosity to use in logging.\n",
      " |      If 0, no output,\n",
      " |      if 1 number of features in current set, if 2 detailed logging i\n",
      " |      ncluding timestamp and cv scores at step.\n",
      " |  scoring : str, callable, or None (default: None)\n",
      " |      If None (default), uses 'accuracy' for sklearn classifiers\n",
      " |      and 'r2' for sklearn regressors.\n",
      " |      If str, uses a sklearn scoring metric string identifier, for example\n",
      " |      {accuracy, f1, precision, recall, roc_auc} for classifiers,\n",
      " |      {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',\n",
      " |      'median_absolute_error', 'r2'} for regressors.\n",
      " |      If a callable object or function is provided, it has to be conform with\n",
      " |      sklearn's signature ``scorer(estimator, X, y)``; see\n",
      " |      http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
      " |      for more information.\n",
      " |  cv : int (default: 5)\n",
      " |      Integer or iterable yielding train, test splits. If cv is an integer\n",
      " |      and `estimator` is a classifier (or y consists of integer class\n",
      " |      labels) stratified k-fold. Otherwise regular k-fold cross-validation\n",
      " |      is performed. No cross-validation if cv is None, False, or 0.\n",
      " |  n_jobs : int (default: 1)\n",
      " |      The number of CPUs to use for evaluating different feature subsets\n",
      " |      in parallel. -1 means 'all CPUs'.\n",
      " |  pre_dispatch : int, or string (default: '2*n_jobs')\n",
      " |      Controls the number of jobs that get dispatched\n",
      " |      during parallel execution if `n_jobs > 1` or `n_jobs=-1`.\n",
      " |      Reducing this number can be useful to avoid an explosion of\n",
      " |      memory consumption when more jobs get dispatched than CPUs can process.\n",
      " |      This parameter can be:\n",
      " |      None, in which case all the jobs are immediately created and spawned.\n",
      " |          Use this for lightweight and fast-running jobs,\n",
      " |          to avoid delays due to on-demand spawning of the jobs\n",
      " |      An int, giving the exact number of total jobs that are spawned\n",
      " |      A string, giving an expression as a function\n",
      " |          of n_jobs, as in `2*n_jobs`\n",
      " |  clone_estimator : bool (default: True)\n",
      " |      Clones estimator if True; works with the original estimator instance\n",
      " |      if False. Set to False if the estimator doesn't\n",
      " |      implement scikit-learn's set_params and get_params methods.\n",
      " |      In addition, it is required to set cv=0, and n_jobs=1.\n",
      " |  fixed_features : tuple (default: None)\n",
      " |      If not `None`, the feature indices provided as a tuple will be\n",
      " |      regarded as fixed by the feature selector. For example, if\n",
      " |      `fixed_features=(1, 3, 7)`, the 2nd, 4th, and 8th feature are\n",
      " |      guaranteed to be present in the solution. Note that if\n",
      " |      `fixed_features` is not `None`, make sure that the number of\n",
      " |      features to be selected is greater than `len(fixed_features)`.\n",
      " |      In other words, ensure that `k_features > len(fixed_features)`.\n",
      " |      New in mlxtend v. 0.18.0.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  k_feature_idx_ : array-like, shape = [n_predictions]\n",
      " |      Feature Indices of the selected feature subsets.\n",
      " |  k_feature_names_ : array-like, shape = [n_predictions]\n",
      " |      Feature names of the selected feature subsets. If pandas\n",
      " |      DataFrames are used in the `fit` method, the feature\n",
      " |      names correspond to the column names. Otherwise, the\n",
      " |      feature names are string representation of the feature\n",
      " |      array indices. New in v 0.13.0.\n",
      " |  k_score_ : float\n",
      " |      Cross validation average score of the selected subset.\n",
      " |  subsets_ : dict\n",
      " |      A dictionary of selected feature subsets during the\n",
      " |      sequential selection, where the dictionary keys are\n",
      " |      the lengths k of these feature subsets. The dictionary\n",
      " |      values are dictionaries themselves with the following\n",
      " |      keys: 'feature_idx' (tuple of indices of the feature subset)\n",
      " |            'feature_names' (tuple of feature names of the feat. subset)\n",
      " |            'cv_scores' (list individual cross-validation scores)\n",
      " |            'avg_score' (average cross-validation score)\n",
      " |      Note that if pandas\n",
      " |      DataFrames are used in the `fit` method, the 'feature_names'\n",
      " |      correspond to the column names. Otherwise, the\n",
      " |      feature names are string representation of the feature\n",
      " |      array indices. The 'feature_names' is new in v 0.13.0.\n",
      " |  \n",
      " |  Examples\n",
      " |  -----------\n",
      " |  For usage examples, please see\n",
      " |  http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SequentialFeatureSelector\n",
      " |      mlxtend.utils.base_compostion._BaseXComposition\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2*n_jobs', clone_estimator=True, fixed_features=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, custom_feature_names=None, groups=None, **fit_params)\n",
      " |      Perform feature selection and learn model from training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |          New in v 0.13.0: pandas DataFrames are now also accepted as\n",
      " |          argument for X.\n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |          New in v 0.13.0: pandas DataFrames are now also accepted as\n",
      " |          argument for y.\n",
      " |      custom_feature_names : None or tuple (default: tuple)\n",
      " |          Custom feature names for `self.k_feature_names` and\n",
      " |          `self.subsets_[i]['feature_names']`.\n",
      " |          (new in v 0.13.0)\n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Passed to the fit method of the cross-validator.\n",
      " |      fit_params : dict of string -> object, optional\n",
      " |          Parameters to pass to to the fit method of classifier.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  fit_transform(self, X, y, groups=None, **fit_params)\n",
      " |      Fit to training data then reduce X to its most important features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |          New in v 0.13.0: pandas DataFrames are now also accepted as\n",
      " |          argument for X.\n",
      " |      y : array-like, shape = [n_samples]\n",
      " |          Target values.\n",
      " |          New in v 0.13.0: a pandas Series are now also accepted as\n",
      " |          argument for y.\n",
      " |      groups : array-like, with shape (n_samples,), optional\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set. Passed to the fit method of the cross-validator.\n",
      " |      fit_params : dict of string -> object, optional\n",
      " |          Parameters to pass to to the fit method of classifier.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Reduced feature subset of X, shape={n_samples, k_features}\n",
      " |  \n",
      " |  get_metric_dict(self, confidence_interval=0.95)\n",
      " |      Return metric dictionary\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      confidence_interval : float (default: 0.95)\n",
      " |          A positive float between 0.0 and 1.0 to compute the confidence\n",
      " |          interval bounds of the CV score averages.\n",
      " |      \n",
      " |      Returns\n",
      " |      ----------\n",
      " |      Dictionary with items where each dictionary value is a list\n",
      " |      with the number of iterations (number of feature subsets) as\n",
      " |      its length. The dictionary keys corresponding to these lists\n",
      " |      are as follows:\n",
      " |          'feature_idx': tuple of the indices of the feature subset\n",
      " |          'cv_scores': list with individual CV scores\n",
      " |          'avg_score': of CV average scores\n",
      " |          'std_dev': standard deviation of the CV score average\n",
      " |          'std_err': standard error of the CV score average\n",
      " |          'ci_bound': confidence interval bound of the CV score average\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      Valid parameter keys can be listed with ``get_params()``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Reduce X to its most important features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |          New in v 0.13.0: pandas DataFrames are now also accepted as\n",
      " |          argument for X.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Reduced feature subset of X, shape={n_samples, k_features}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  named_estimators\n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of named estimator tuples, like [('svc', SVC(...))]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      " |  \n",
      " |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SequentialFeatureSelector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CRIM', 'CHAS', 'RM', 'PTRATIO', 'LSTAT')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = SequentialFeatureSelector(model, k_features=5, n_jobs=-1)\n",
    "fs.fit(x, y)\n",
    "fs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'feature_idx': (12,),\n",
       "  'cv_scores': array([0.31784807, 0.5406078 , 0.07608699, 0.42423767, 0.1267687 ]),\n",
       "  'avg_score': 0.2971098460066865,\n",
       "  'feature_names': ('LSTAT',)},\n",
       " 2: {'feature_idx': (10, 12),\n",
       "  'cv_scores': array([0.47872024, 0.55896066, 0.29416107, 0.44039936, 0.37716673]),\n",
       "  'avg_score': 0.4298816135857271,\n",
       "  'feature_names': ('PTRATIO', 'LSTAT')},\n",
       " 3: {'feature_idx': (3, 10, 12),\n",
       "  'cv_scores': array([0.5125899 , 0.52887883, 0.31209504, 0.47271838, 0.37935597]),\n",
       "  'avg_score': 0.4411276233882674,\n",
       "  'feature_names': ('CHAS', 'PTRATIO', 'LSTAT')},\n",
       " 4: {'feature_idx': (3, 5, 10, 12),\n",
       "  'cv_scores': array([0.74342797, 0.68528955, 0.55392829, 0.20544105, 0.02541525]),\n",
       "  'avg_score': 0.4427004222330956,\n",
       "  'feature_names': ('CHAS', 'RM', 'PTRATIO', 'LSTAT')},\n",
       " 5: {'feature_idx': (0, 3, 5, 10, 12),\n",
       "  'cv_scores': array([0.74205803, 0.69167982, 0.55264435, 0.19713656, 0.06081025]),\n",
       "  'avg_score': 0.4488658019850355,\n",
       "  'feature_names': ('CRIM', 'CHAS', 'RM', 'PTRATIO', 'LSTAT')}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward stepwise selection\n",
    "---\n",
    "最も評価指標に影響しない特徴を 1 つずつ削除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pythonによるbackward stepwise selectionの実行方法\n",
    "---\n",
    "`mlxtend.feature_selection.SequentialFeatureSelector`の引数`forward=False`にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CRIM',\n",
       " 'ZN',\n",
       " 'CHAS',\n",
       " 'NOX',\n",
       " 'AGE',\n",
       " 'DIS',\n",
       " 'RAD',\n",
       " 'TAX',\n",
       " 'PTRATIO',\n",
       " 'B',\n",
       " 'LSTAT')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = SequentialFeatureSelector(model,\n",
    "                               k_features=x.columns.size - 2,\n",
    "                               forward=False,\n",
    "                               n_jobs=-1)\n",
    "bs.fit(x, y)\n",
    "bs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{13: {'feature_idx': (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),\n",
       "  'cv_scores': array([ 0.63919994,  0.71386698,  0.58702344,  0.07923081, -0.25294154]),\n",
       "  'avg_score': 0.3532759243958797,\n",
       "  'feature_names': ('CRIM',\n",
       "   'ZN',\n",
       "   'INDUS',\n",
       "   'CHAS',\n",
       "   'NOX',\n",
       "   'RM',\n",
       "   'AGE',\n",
       "   'DIS',\n",
       "   'RAD',\n",
       "   'TAX',\n",
       "   'PTRATIO',\n",
       "   'B',\n",
       "   'LSTAT')},\n",
       " 12: {'feature_idx': (0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12),\n",
       "  'cv_scores': array([0.46450601, 0.61522064, 0.43616681, 0.57879898, 0.37296639]),\n",
       "  'avg_score': 0.4935317652178909,\n",
       "  'feature_names': ('CRIM',\n",
       "   'ZN',\n",
       "   'INDUS',\n",
       "   'CHAS',\n",
       "   'NOX',\n",
       "   'AGE',\n",
       "   'DIS',\n",
       "   'RAD',\n",
       "   'TAX',\n",
       "   'PTRATIO',\n",
       "   'B',\n",
       "   'LSTAT')},\n",
       " 11: {'feature_idx': (0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 12),\n",
       "  'cv_scores': array([0.46910347, 0.66682374, 0.43729546, 0.5814138 , 0.37173933]),\n",
       "  'avg_score': 0.5052751616354444,\n",
       "  'feature_names': ('CRIM',\n",
       "   'ZN',\n",
       "   'CHAS',\n",
       "   'NOX',\n",
       "   'AGE',\n",
       "   'DIS',\n",
       "   'RAD',\n",
       "   'TAX',\n",
       "   'PTRATIO',\n",
       "   'B',\n",
       "   'LSTAT')}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permutation importance\n",
    "---\n",
    "ある特徴の値をランダムに並べ替えて、元のデータでの評価指標と並べ替えたデータでの評価指標の差によって特徴の重要度を決定する。\n",
    "\n",
    "<table class=\"border text-center\">\n",
    "    <tr class=\"border-none background-default\">\n",
    "        <th colspan=\"3\">元データ</th>\n",
    "        <th rowspan=\"7\"></th>\n",
    "        <th colspan=\"3\">特徴Aの評価用</th>\n",
    "        <th rowspan=\"7\"></th>\n",
    "        <th colspan=\"3\">特徴Bの評価用</th>\n",
    "        <th rowspan=\"7\"></th>\n",
    "        <th colspan=\"3\">特徴Cの評価用</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>特徴A</th>\n",
    "        <th>特徴B</th>\n",
    "        <th>特徴C</th>\n",
    "        <th>特徴A</th>\n",
    "        <th>特徴B</th>\n",
    "        <th>特徴C</th>\n",
    "        <th>特徴A</th>\n",
    "        <th>特徴B</th>\n",
    "        <th>特徴C</th>\n",
    "        <th>特徴A</th>\n",
    "        <th>特徴B</th>\n",
    "        <th>特徴C</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$A_{1}$</td>\n",
    "        <td>$B_{1}$</td>\n",
    "        <td>$C_{1}$</td>\n",
    "        <td>$A_{2}$</td>\n",
    "        <td>$B_{1}$</td>\n",
    "        <td>$C_{1}$</td>\n",
    "        <td>$A_{1}$</td>\n",
    "        <td>$B_{3}$</td>\n",
    "        <td>$C_{1}$</td>\n",
    "        <td>$A_{1}$</td>\n",
    "        <td>$B_{1}$</td>\n",
    "        <td>$C_{3}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$A_{2}$</td>\n",
    "        <td>$B_{2}$</td>\n",
    "        <td>$C_{2}$</td>\n",
    "        <td>$A_{3}$</td>\n",
    "        <td>$B_{2}$</td>\n",
    "        <td>$C_{2}$</td>\n",
    "        <td>$A_{2}$</td>\n",
    "        <td>$B_{1}$</td>\n",
    "        <td>$C_{2}$</td>\n",
    "        <td>$A_{2}$</td>\n",
    "        <td>$B_{2}$</td>\n",
    "        <td>$C_{1}$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$A_{3}$</td>\n",
    "        <td>$B_{3}$</td>\n",
    "        <td>$C_{3}$</td>\n",
    "        <td>$A_{1}$</td>\n",
    "        <td>$B_{3}$</td>\n",
    "        <td>$C_{3}$</td>\n",
    "        <td>$A_{3}$</td>\n",
    "        <td>$B_{2}$</td>\n",
    "        <td>$C_{3}$</td>\n",
    "        <td>$A_{3}$</td>\n",
    "        <td>$B_{3}$</td>\n",
    "        <td>$C_{2}$</td>\n",
    "    </tr>\n",
    "    <tr class=\"border-none background-default\">\n",
    "        <td colspan=\"3\">↓</td>\n",
    "        <td colspan=\"3\">↓</td>\n",
    "        <td colspan=\"3\">↓</td>\n",
    "        <td colspan=\"3\">↓</td>\n",
    "    </tr>\n",
    "    <tr class=\"border-none background-default\">\n",
    "        <td colspan=\"3\">$accuracy$</td>\n",
    "        <td colspan=\"3\">$accuracy_{A}$</td>\n",
    "        <td colspan=\"3\">$accuracy_{B}$</td>\n",
    "        <td colspan=\"3\">$accuracy_{C}$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythonによるpermutation importanceの実行方法\n",
    "---\n",
    "`sklearn.inspection.permutation_importance`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function permutation_importance in module sklearn.inspection._permutation_importance:\n",
      "\n",
      "permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None)\n",
      "    Permutation importance for feature evaluation [BRE]_.\n",
      "    \n",
      "    The :term:`estimator` is required to be a fitted estimator. `X` can be the\n",
      "    data set used to train the estimator or a hold-out set. The permutation\n",
      "    importance of a feature is calculated as follows. First, a baseline metric,\n",
      "    defined by :term:`scoring`, is evaluated on a (potentially different)\n",
      "    dataset defined by the `X`. Next, a feature column from the validation set\n",
      "    is permuted and the metric is evaluated again. The permutation importance\n",
      "    is defined to be the difference between the baseline metric and metric from\n",
      "    permutating the feature column.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <permutation_importance>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : object\n",
      "        An estimator that has already been :term:`fitted` and is compatible\n",
      "        with :term:`scorer`.\n",
      "    \n",
      "    X : ndarray or DataFrame, shape (n_samples, n_features)\n",
      "        Data on which permutation importance will be computed.\n",
      "    \n",
      "    y : array-like or None, shape (n_samples, ) or (n_samples, n_classes)\n",
      "        Targets for supervised or `None` for unsupervised.\n",
      "    \n",
      "    scoring : string, callable or None, default=None\n",
      "        Scorer to use. It can be a single\n",
      "        string (see :ref:`scoring_parameter`) or a callable (see\n",
      "        :ref:`scoring`). If None, the estimator's default scorer is used.\n",
      "    \n",
      "    n_repeats : int, default=5\n",
      "        Number of times to permute a feature.\n",
      "    \n",
      "    n_jobs : int or None, default=None\n",
      "        The number of jobs to use for the computation.\n",
      "        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "    \n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Pseudo-random number generator to control the permutations of each\n",
      "        feature.\n",
      "        Pass an int to get reproducible results across function calls.\n",
      "        See :term: `Glossary <random_state>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        importances_mean : ndarray, shape (n_features, )\n",
      "            Mean of feature importance over `n_repeats`.\n",
      "        importances_std : ndarray, shape (n_features, )\n",
      "            Standard deviation over `n_repeats`.\n",
      "        importances : ndarray, shape (n_features, n_repeats)\n",
      "            Raw permutation importance scores.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [BRE] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32,\n",
      "             2001. https://doi.org/10.1023/A:1010933404324\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> from sklearn.inspection import permutation_importance\n",
      "    >>> X = [[1, 9, 9],[1, 9, 9],[1, 9, 9],\n",
      "    ...      [0, 9, 9],[0, 9, 9],[0, 9, 9]]\n",
      "    >>> y = [1, 1, 1, 0, 0, 0]\n",
      "    >>> clf = LogisticRegression().fit(X, y)\n",
      "    >>> result = permutation_importance(clf, X, y, n_repeats=10,\n",
      "    ...                                 random_state=0)\n",
      "    >>> result.importances_mean\n",
      "    array([0.4666..., 0.       , 0.       ])\n",
      "    >>> result.importances_std\n",
      "    array([0.2211..., 0.       , 0.       ])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(permutation_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'importances_mean': array([ 1.96611776e-02,  2.88784136e-02,  5.17942896e-04,  1.16984535e-02,\n",
       "         9.51127766e-02,  1.65389569e-01, -3.92127478e-05,  2.22342246e-01,\n",
       "         1.68006784e-01,  9.81300283e-02,  1.05044328e-01,  1.44056929e-02,\n",
       "         3.33326050e-01]),\n",
       " 'importances_std': array([2.82530773e-03, 3.24397080e-03, 3.71467778e-04, 3.73854063e-03,\n",
       "        7.05170892e-03, 1.46838030e-02, 5.54020404e-05, 1.58087709e-02,\n",
       "        1.38840358e-02, 8.67227171e-03, 1.26760553e-02, 4.40860777e-03,\n",
       "        2.45306602e-02]),\n",
       " 'importances': array([[ 2.02354023e-02,  2.03645514e-02,  2.34847108e-02,\n",
       "          1.47171653e-02,  1.95040583e-02],\n",
       "        [ 2.45676015e-02,  2.85112681e-02,  3.46400809e-02,\n",
       "          2.85761550e-02,  2.80969625e-02],\n",
       "        [ 8.86440184e-04,  4.26798509e-04, -5.70749560e-05,\n",
       "          9.59090491e-04,  3.74460251e-04],\n",
       "        [ 9.26068758e-03,  1.35827993e-02,  1.81064411e-02,\n",
       "          7.72482256e-03,  9.81751722e-03],\n",
       "        [ 8.34655791e-02,  1.04692003e-01,  9.39349143e-02,\n",
       "          9.40376271e-02,  9.94337593e-02],\n",
       "        [ 1.73182033e-01,  1.42242401e-01,  1.83044354e-01,\n",
       "          1.55095683e-01,  1.73383376e-01],\n",
       "        [-1.55078752e-05, -9.01451034e-05,  5.20097966e-05,\n",
       "         -4.10302519e-05, -1.01390305e-04],\n",
       "        [ 2.46958056e-01,  2.12682842e-01,  2.16851654e-01,\n",
       "          2.33026807e-01,  2.02191872e-01],\n",
       "        [ 1.85250619e-01,  1.71108782e-01,  1.57892051e-01,\n",
       "          1.78737599e-01,  1.47044871e-01],\n",
       "        [ 8.98333186e-02,  9.25512997e-02,  1.08659365e-01,\n",
       "          9.08694699e-02,  1.08736688e-01],\n",
       "        [ 1.09861573e-01,  8.93544697e-02,  1.22730248e-01,\n",
       "          9.16869956e-02,  1.11588355e-01],\n",
       "        [ 1.10004417e-02,  1.69244258e-02,  2.19459460e-02,\n",
       "          1.14308567e-02,  1.07267943e-02],\n",
       "        [ 3.35093025e-01,  3.11988491e-01,  3.48083057e-01,\n",
       "          3.01666738e-01,  3.69798938e-01]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y)\n",
    "permutation_importance(model, x, y, n_jobs=-1, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの性質に基づくもの\n",
    "---\n",
    "モデルによっては学習後に特徴の重要性を表す情報が得られるので、それを活用して特徴を選択する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso回帰\n",
    "---\n",
    "[L1 正則化](regularization.ipynb#L1%E6%AD%A3%E5%89%87%E5%8C%96)を用いる Lasso 回帰は、係数が 0 になった特徴は重要でないと考えられるので、特徴選択に利用可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダム・フォレスト (random forest)\n",
    "---\n",
    "一部の特徴のみ (サンプルも一部のみの場合あり) を使って学習した複数の決定木の多数決や平均値で予測する手法。集団学習 (アンサンブル学習) の中のバギング (bagging) の代表的手法。\n",
    "\n",
    "<table class=\"border text-center\">\n",
    "    <tr class=\"border-none background-default\">\n",
    "        <th colspan=\"4\">決定木1</th>\n",
    "        <td rowspan=\"5\"></td>\n",
    "        <th colspan=\"4\">決定木2</th>\n",
    "        <td rowspan=\"5\"></td>\n",
    "        <th colspan=\"4\">決定木3</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th class=\"border-top-none border-right-bold background-default\"></th>\n",
    "        <th class=\"background-dark text-red\">特徴A</th>\n",
    "        <th class=\"background-dark text-red\">特徴B</th>\n",
    "        <th class=\"background-dark\">特徴C</th>\n",
    "        <th class=\"border-top-none border-right-bold background-default\"></th>\n",
    "        <th class=\"background-dark\">特徴A</th>\n",
    "        <th class=\"background-dark text-red\">特徴B</th>\n",
    "        <th class=\"background-dark text-red\">特徴C</th>\n",
    "        <th class=\"border-top-none border-right-bold background-default\"></th>\n",
    "        <th class=\"background-dark text-red\">特徴A</th>\n",
    "        <th class=\"background-dark\">特徴B</th>\n",
    "        <th class=\"background-dark text-red\">特徴C</th>\n",
    "    </tr>\n",
    "    <tr class=\"background-bright\">\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル1</th>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td>不使用</td>\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル1</th>\n",
    "        <td>不使用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <th class=\"border-bottom border-right-bold background-dark\">サンプル1</th>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "    </tr>\n",
    "    <tr class=\"background-bright\">\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル2</th>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td>不使用</td>\n",
    "        <th class=\"border-bottom border-right-bold background-dark\">サンプル2</th>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル2</th>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td>不使用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "    </tr>\n",
    "    <tr class=\"background-bright\">\n",
    "        <th class=\"border-bottom border-right-bold background-dark\">サンプル3</th>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "        <td>不使用</td>\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル3</th>\n",
    "        <td>不使用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <th class=\"border-bottom border-right-bold text-red background-dark\">サンプル3</th>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "        <td>不使用</td>\n",
    "        <td class=\"text-red\">利用</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 他の手法との比較\n",
    "---\n",
    "[決定木](decision_tree.ipynb)は、十分な深さがあればほぼ全てのサンプルを完璧に分類できる代わりに汎化性能が著しく低くなる高バリアンスなモデル。ランダムフォレストは、一部の特徴・一部のサンプルだけを使った決定木を多数構築し、それらの決定木の多数決で予測することで汎化性能を高める。ただし、決定木のような結果に対する説明力はない。\n",
    "\n",
    "Lasso 回帰は説明変数と目的変数の関係が線形であることを前提にしているが、ランダム・フォレストは決定木を用いるので線形な関係でなくともよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 仕組み\n",
    "---\n",
    "scikit-learn の実装では、その特徴が分岐条件に使われた際の (そのノードに到達するサンプルの割合で加重された) 不純度の平均的な減少量によって特徴の重要度を決定する。\n",
    "\n",
    "多重共線性 (特徴の相関が高い) などの場合には、一方の重要度が高く、他方が低くなることもあるが、特徴選択に用いるだけならそれほど問題ない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from my_functions.feature_selection import rf_feature_importance\n",
    "rf_feature_importance.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythonでのランダム・フォレストによる特徴選択の実行方法\n",
    "---\n",
    "`sklearn.ensemble.RandomForestClassifier`や`sklearn.ensemble.RandomForestRegressor`を学習させた後、`feature_importances_`属性を参照する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cancer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>...</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  ...  \\\n",
       "0          17.99         10.38          122.80     1001.0  ...   \n",
       "1          20.57         17.77          132.90     1326.0  ...   \n",
       "..           ...           ...             ...        ...  ...   \n",
       "567        20.60         29.33          140.10     1265.0  ...   \n",
       "568         7.76         24.54           47.92      181.0  ...   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890     0.0  \n",
       "1                  0.1860          0.2750                  0.08902     0.0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "567                0.2650          0.4087                  0.12400     0.0  \n",
       "568                0.0000          0.2871                  0.07039     1.0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = load_breast_cancer()\n",
    "cancer = pd.DataFrame(np.column_stack([loader.data, loader.target]),\n",
    "                      columns=list(loader.feature_names) + ['target'])\n",
    "print('cancer')\n",
    "display(cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0572391 , 0.01818295, 0.04976017, 0.03476067, 0.00546018,\n",
       "       0.00846855, 0.05699552, 0.08295488, 0.00533678, 0.00325888,\n",
       "       0.00600567, 0.005381  , 0.00951609, 0.03304585, 0.00503519,\n",
       "       0.00529307, 0.00619566, 0.00701214, 0.00314792, 0.00486091,\n",
       "       0.11535804, 0.01681525, 0.16187602, 0.10815229, 0.01282889,\n",
       "       0.01485554, 0.02634281, 0.11694055, 0.01228586, 0.00663357])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cancer.iloc[:, :-1]\n",
    "y = cancer.iloc[:, -1]\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=1234)\n",
    "rf.fit(x, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['worst perimeter', 'worst concave points', 'worst radius', 'worst area',\n",
       "       'mean concave points', 'mean radius', 'mean concavity',\n",
       "       'mean perimeter', 'mean area', 'area error', 'worst concavity',\n",
       "       'mean texture', 'worst texture', 'worst compactness',\n",
       "       'worst smoothness', 'worst symmetry', 'perimeter error',\n",
       "       'mean compactness', 'concave points error', 'worst fractal dimension',\n",
       "       'concavity error', 'radius error', 'mean smoothness', 'texture error',\n",
       "       'mean symmetry', 'compactness error', 'smoothness error',\n",
       "       'fractal dimension error', 'mean fractal dimension', 'symmetry error'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[np.argsort(rf.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive feature elimination\n",
    "---\n",
    "重み (パラメーター) の大きさが変数の重要度を表すモデルや`feature_importances_`属性を持つモデルを学習させ、重要度の小さい特徴から 1 つずつ削除。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythonでのrecursive feature eliminationの実行方法\n",
    "---\n",
    "`sklearn.feature_selection.RFE`を使用する。利用可能なモデルは`coef_`または`feature_importances_`属性を持つもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>...</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS  ...  PTRATIO       B  LSTAT  target\n",
       "0    0.00632  18.0   2.31   0.0  ...     15.3  396.90   4.98    24.0\n",
       "1    0.02731   0.0   7.07   0.0  ...     17.8  396.90   9.14    21.6\n",
       "..       ...   ...    ...   ...  ...      ...     ...    ...     ...\n",
       "504  0.10959   0.0  11.93   0.0  ...     21.0  393.45   6.48    22.0\n",
       "505  0.04741   0.0  11.93   0.0  ...     21.0  396.90   7.88    11.9\n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('boston')\n",
    "display(boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RFE in module sklearn.feature_selection._rfe:\n",
      "\n",
      "class RFE(sklearn.feature_selection._base.SelectorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      " |  RFE(estimator, *, n_features_to_select=None, step=1, verbose=0)\n",
      " |  \n",
      " |  Feature ranking with recursive feature elimination.\n",
      " |  \n",
      " |  Given an external estimator that assigns weights to features (e.g., the\n",
      " |  coefficients of a linear model), the goal of recursive feature elimination\n",
      " |  (RFE) is to select features by recursively considering smaller and smaller\n",
      " |  sets of features. First, the estimator is trained on the initial set of\n",
      " |  features and the importance of each feature is obtained either through a\n",
      " |  ``coef_`` attribute or through a ``feature_importances_`` attribute.\n",
      " |  Then, the least important features are pruned from current set of features.\n",
      " |  That procedure is recursively repeated on the pruned set until the desired\n",
      " |  number of features to select is eventually reached.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <rfe>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  estimator : object\n",
      " |      A supervised learning estimator with a ``fit`` method that provides\n",
      " |      information about feature importance either through a ``coef_``\n",
      " |      attribute or through a ``feature_importances_`` attribute.\n",
      " |  \n",
      " |  n_features_to_select : int or None (default=None)\n",
      " |      The number of features to select. If `None`, half of the features\n",
      " |      are selected.\n",
      " |  \n",
      " |  step : int or float, optional (default=1)\n",
      " |      If greater than or equal to 1, then ``step`` corresponds to the\n",
      " |      (integer) number of features to remove at each iteration.\n",
      " |      If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
      " |      (rounded down) of features to remove at each iteration.\n",
      " |  \n",
      " |  verbose : int, (default=0)\n",
      " |      Controls verbosity of output.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_features_ : int\n",
      " |      The number of selected features.\n",
      " |  \n",
      " |  support_ : array of shape [n_features]\n",
      " |      The mask of selected features.\n",
      " |  \n",
      " |  ranking_ : array of shape [n_features]\n",
      " |      The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
      " |      ranking position of the i-th feature. Selected (i.e., estimated\n",
      " |      best) features are assigned rank 1.\n",
      " |  \n",
      " |  estimator_ : object\n",
      " |      The external estimator fit on the reduced dataset.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to retrieve the 5 most informative\n",
      " |  features in the Friedman #1 dataset.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_friedman1\n",
      " |  >>> from sklearn.feature_selection import RFE\n",
      " |  >>> from sklearn.svm import SVR\n",
      " |  >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
      " |  >>> estimator = SVR(kernel=\"linear\")\n",
      " |  >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
      " |  >>> selector = selector.fit(X, y)\n",
      " |  >>> selector.support_\n",
      " |  array([ True,  True,  True,  True,  True, False, False, False, False,\n",
      " |         False])\n",
      " |  >>> selector.ranking_\n",
      " |  array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Allows NaN/Inf in the input if the underlying estimator does as well.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  RFECV : Recursive feature elimination with built-in cross-validated\n",
      " |      selection of the best number of features\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
      " |         for cancer classification using support vector machines\",\n",
      " |         Mach. Learn., 46(1-3), 389--422, 2002.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RFE\n",
      " |      sklearn.feature_selection._base.SelectorMixin\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, *, n_features_to_select=None, step=1, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : array, shape = [n_samples, n_classes] or [n_samples]\n",
      " |          The decision function of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification produce an array of shape\n",
      " |          [n_samples].\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the RFE model and then the underlying estimator on the selected\n",
      " |         features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Reduce X to the selected features and then predict using the\n",
      " |         underlying estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape [n_samples]\n",
      " |          The predicted target values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  score(self, X, y)\n",
      " |      Reduce X to the selected features and then return the score of the\n",
      " |         underlying estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      y : array of shape [n_samples]\n",
      " |          The target values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  classes_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      " |  \n",
      " |  get_support(self, indices=False)\n",
      " |      Get a mask, or integer index, of the features selected\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indices : boolean (default False)\n",
      " |          If True, the return value will be an array of integers, rather\n",
      " |          than a boolean mask.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      support : array\n",
      " |          An index that selects the retained features from a feature vector.\n",
      " |          If `indices` is False, this is a boolean array of shape\n",
      " |          [# input features], in which an element is True iff its\n",
      " |          corresponding feature is selected for retention. If `indices` is\n",
      " |          True, this is an integer array of shape [# output features] whose\n",
      " |          values are indices into the input feature vector.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Reverse the transformation operation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_original_features]\n",
      " |          `X` with columns of zeros inserted where features would have\n",
      " |          been removed by :meth:`transform`.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Reduce X to the selected features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array of shape [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_r : array of shape [n_samples, n_selected_features]\n",
      " |          The input samples with only the selected features.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 10,  9,  3,  1,  2, 13,  5,  7, 11,  4, 12,  6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = boston.iloc[:, :-1]\n",
    "y = boston.iloc[:, -1]\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=1)\n",
    "rfe.fit(x, y)\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NOX', 'RM', 'CHAS', 'PTRATIO', 'DIS', 'LSTAT', 'RAD', 'CRIM', 'INDUS',\n",
       "       'ZN', 'TAX', 'B', 'AGE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[np.argsort(rfe.ranking_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### その他の手法\n",
    "---\n",
    "勾配ブースティングという手法を実装した[LightGBM](https://lightgbm.readthedocs.io/en/latest/)や[xgboost](https://xgboost.readthedocs.io/en/latest/)は欠損値があっても動作し、ランダム・フォレストのように`feature_importances_`で特徴の重要度が参照できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LightGBM`の使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([207,  14,  69,  11, 138, 269, 216, 253,  49,  79,  99, 185, 293],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgbm = LGBMRegressor(random_state=1234)\n",
    "model_lgbm.fit(x, y)\n",
    "model_lgbm.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LSTAT', 'RM', 'DIS', 'AGE', 'CRIM', 'B', 'NOX', 'PTRATIO', 'TAX',\n",
       "       'INDUS', 'RAD', 'ZN', 'CHAS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[np.argsort(model_lgbm.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgboost`の使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01778572, 0.00137419, 0.01510167, 0.00365362, 0.0518821 ,\n",
       "       0.23821141, 0.00902774, 0.04952978, 0.01097363, 0.03895662,\n",
       "       0.04516473, 0.00857165, 0.50976723], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBRegressor(random_state=1234)\n",
    "model_xgb.fit(x, y)\n",
    "model_xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LSTAT', 'RM', 'NOX', 'DIS', 'PTRATIO', 'TAX', 'CRIM', 'INDUS', 'RAD',\n",
       "       'AGE', 'B', 'CHAS', 'ZN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[np.argsort(model_xgb.feature_importances_)[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推薦図書\n",
    "---\n",
    "- [Python 機械学習プログラミング 達人データサイエンティストによる理論と実践](https://www.amazon.co.jp/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E9%81%94%E4%BA%BA%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E3%81%AB%E3%82%88%E3%82%8B%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%B7%B5-impress-gear/dp/4295003379/)\n",
    "- [Kaggleで勝つデータ分析の技術](https://www.amazon.co.jp/Kaggle%E3%81%A7%E5%8B%9D%E3%81%A4%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E6%8A%80%E8%A1%93-%E9%96%80%E8%84%87-%E5%A4%A7%E8%BC%94/dp/4297108437/)\n",
    "- [Pythonではじめる機械学習 ―scikit-learnで学ぶ特徴量エンジニアリングと機械学習の基礎](https://www.amazon.co.jp/Python%E3%81%A7%E3%81%AF%E3%81%98%E3%82%81%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92-%E2%80%95scikit-learn%E3%81%A7%E5%AD%A6%E3%81%B6%E7%89%B9%E5%BE%B4%E9%87%8F%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E5%9F%BA%E7%A4%8E-Andreas-C-Muller/dp/4873117984/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
