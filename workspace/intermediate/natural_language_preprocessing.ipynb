{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from tensorflow.keras.utils import get_file\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import spacy.lang.ja\n",
    "import MeCab\n",
    "from pyknp import Juman\n",
    "from IPython.display import display\n",
    "pd.set_option('max_rows', 5)\n",
    "pd.set_option('max_columns', 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語の前処理\n",
    "---\n",
    "人間が日常の意思疎通のために用いている言語のことを自然言語という。  \n",
    "自然言語は前処理を施さないと分析に適しないが、前処理の方法は言語ごとに異なる。\n",
    "\n",
    "- 一般的な処理手順\n",
    " 1. 形態素解析 (文書を単語単位に分割+単語の変化形などを1つにまとめる)\n",
    "   - 英語の場合は[NLTK](https://www.nltk.org/)や[gensim](https://radimrehurek.com/gensim/)を使用し、分割してから[stemming](https://en.wikipedia.org/wiki/Stemming)や[lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)を実施\n",
    "   - 日本語の場合は[spaCy](https://spacy.io/)+[GiNZA](https://megagonlabs.github.io/ginza/)・[MeCab](https://taku910.github.io/mecab/)・[JUMAN++](http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN++)などを使用\n",
    " 1. ストップワードを除去\n",
    "   - どんな文書にも頻繁に現れ、文書の特徴を表すのに役立たない単語 (\"I\", \"is\"や\"私\"、\"です\"など)を除去\n",
    "   - 通常は言語ごとのストップワードの辞書を使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\n\\nLikewise for me please. First time I've ...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sorry, but I just wanted to be the first hypoc...</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>\\nPut up or shut up. Where is your evidence?\\n...</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>\\n\\n\\n\\nWe're looking at a series of chips by ...</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document            category\n",
       "0      \\n\\n\\nLikewise for me please. First time I've ...       comp.graphics\n",
       "1      Sorry, but I just wanted to be the first hypoc...  talk.politics.misc\n",
       "...                                                  ...                 ...\n",
       "18844  \\nPut up or shut up. Where is your evidence?\\n...  talk.politics.misc\n",
       "18845  \\n\\n\\n\\nWe're looking at a series of chips by ...     sci.electronics\n",
       "\n",
       "[18846 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "loader = fetch_20newsgroups(subset='all', random_state=1234, remove=remove)\n",
    "news = pd.DataFrame(dict(document=loader.data, category=loader.target))\n",
    "news['category'] = pd.Categorical.from_codes(news['category'],\n",
    "                                             categories=loader.target_names)\n",
    "print('news')\n",
    "display(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "---\n",
    "ある文書における各単語の出現回数を数え、それをその文書の特徴とする手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PythonでのBag of Words作成方法\n",
    "---\n",
    "英語の場合は、`sklearn.feature_extraction.text.CountVectorizer`や`gensim.corpora.Dictionary.doc2bow`を使用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn`の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}, default='content'\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 8061 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaa  aaron  ab  abandon  abandoned  ...  zionist  zionists  zip  \\\n",
       "0       0    0      0   0        0          0  ...        0         0    0   \n",
       "1       0    0      0   0        0          0  ...        0         0    0   \n",
       "...    ..  ...    ...  ..      ...        ...  ...      ...       ...  ...   \n",
       "18844   0    0      0   0        0          0  ...        0         0    0   \n",
       "18845   0    0      0   0        0          0  ...        0         0    0   \n",
       "\n",
       "       zone  zoom  zx  \n",
       "0         0     0   0  \n",
       "1         0     0   0  \n",
       "...     ...   ...  ..  \n",
       "18844     0     0   0  \n",
       "18845     0     0   0  \n",
       "\n",
       "[18846 rows x 8061 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ノイズが多いのでアルファベットのみで構成され、文書全体で20回以上出現する単語に限定\n",
    "vectorizer = CountVectorizer(stop_words='english',\n",
    "                             token_pattern='(?u)\\\\b[a-z][a-z]+\\\\b',\n",
    "                             min_df=20)\n",
    "vec = vectorizer.fit_transform(news['document'])\n",
    "news_bow = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vec, columns=vectorizer.get_feature_names())\n",
    "news_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_sample(df, original_data):\n",
    "    exists = df.iloc[0] > 0\n",
    "    display(pd.DataFrame().append([df.iloc[0, list(exists)]]))\n",
    "    print(original_data.loc[0, 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hear</th>\n",
       "      <th>like</th>\n",
       "      <th>likewise</th>\n",
       "      <th>looking</th>\n",
       "      <th>months</th>\n",
       "      <th>past</th>\n",
       "      <th>time</th>\n",
       "      <th>ve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hear  like  likewise  looking  months  past  time  ve\n",
       "0     1     1         1        1       1     1     1   2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Likewise for me please. First time I've hear of it, but I've beem looking\n",
      "for something like this for the past few months.\n"
     ]
    }
   ],
   "source": [
    "check_first_sample(news_bow, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk`+`gensim`の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function doc2bow in module gensim.corpora.dictionary:\n",
      "\n",
      "doc2bow(self, document, allow_update=False, return_missing=False)\n",
      "    Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    document : list of str\n",
      "        Input document.\n",
      "    allow_update : bool, optional\n",
      "        Update self, by adding new tokens from `document` and updating internal corpus statistics.\n",
      "    return_missing : bool, optional\n",
      "        Return missing tokens (tokens present in `document` but not in self) with frequencies?\n",
      "    \n",
      "    Return\n",
      "    ------\n",
      "    list of (int, int)\n",
      "        BoW representation of `document`.\n",
      "    list of (int, int), dict of (str, int)\n",
      "        If `return_missing` is True, return BoW representation of `document` + dictionary with missing\n",
      "        tokens and their frequencies.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.corpora import Dictionary\n",
      "        >>> dct = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
      "        >>> dct.doc2bow([\"this\", \"is\", \"máma\"])\n",
      "        [(2, 1)]\n",
      "        >>> dct.doc2bow([\"this\", \"is\", \"máma\"], return_missing=True)\n",
      "        ([(2, 1)], {u'this': 1, u'is': 1})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.corpora.Dictionary.doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2matrix(documents):\n",
    "    dictionary = gensim.corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    matrix = gensim.matutils.corpus2csc(corpus, num_terms=len(dictionary)).T\n",
    "    dictionary[0]  # dictionary.id2tokenの作成に必要\n",
    "    bow_matrix = pd.DataFrame.sparse.from_spmatrix(\n",
    "        matrix.astype(int),\n",
    "        columns=[dictionary.id2token[i] for i in range(len(dictionary))])\n",
    "    return bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'ve</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>First</th>\n",
       "      <th>I</th>\n",
       "      <th>Likewise</th>\n",
       "      <th>...</th>\n",
       "      <th>congo</th>\n",
       "      <th>GROWN</th>\n",
       "      <th>PSD3xx</th>\n",
       "      <th>WSI</th>\n",
       "      <th>_mega_</th>\n",
       "      <th>sourcing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 217168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       've  ,  .  First  I  Likewise  ...  congo  GROWN  PSD3xx  WSI  _mega_  \\\n",
       "0        2  1  2      1  2         1  ...      0      0       0    0       0   \n",
       "1        0  1  2      0  3         0  ...      0      0       0    0       0   \n",
       "...    ... .. ..    ... ..       ...  ...    ...    ...     ...  ...     ...   \n",
       "18844    0  1  5      0  1         0  ...      0      1       0    0       0   \n",
       "18845    0  6  3      0  0         0  ...      0      0       1    1       1   \n",
       "\n",
       "       sourcing  \n",
       "0             0  \n",
       "1             0  \n",
       "...         ...  \n",
       "18844         0  \n",
       "18845         1  \n",
       "\n",
       "[18846 rows x 217168 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "docs = []\n",
    "for document in news['document']:\n",
    "    words = [\n",
    "        w for w in nltk.tokenize.word_tokenize(document) if w not in stop_words\n",
    "    ]\n",
    "    docs.append(words)\n",
    "news_bow = doc2matrix(docs)\n",
    "news_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'ve</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>First</th>\n",
       "      <th>I</th>\n",
       "      <th>Likewise</th>\n",
       "      <th>...</th>\n",
       "      <th>looking</th>\n",
       "      <th>months</th>\n",
       "      <th>past</th>\n",
       "      <th>please</th>\n",
       "      <th>something</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   've  ,  .  First  I  Likewise  ...  looking  months  past  please  \\\n",
       "0    2  1  2      1  2         1  ...        1       1     1       1   \n",
       "\n",
       "   something  time  \n",
       "0          1     1  \n",
       "\n",
       "[1 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Likewise for me please. First time I've hear of it, but I've beem looking\n",
      "for something like this for the past few months.\n"
     ]
    }
   ],
   "source": [
    "check_first_sample(news_bow, news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語の場合は、`GiNZA`・`mecab`・`JUMAN++`などで形態素解析を行なってから、`gensim`を使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sangokushi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>三国志は、いうまでもなく、今から約千八百年前の古典であるが、三国志の中に活躍している登場人物...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>三国志には、詩がある。\\r\\n　単に尨大な治乱興亡を記述した戦記軍談の類でない所に、東洋人の...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>現在の地名と、原本の誌す地名とは、当然時代による異いがあるので、分っている地方は下に註を加え...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>原本には「通俗三国志」「三国志演義」その他数種あるが、私はそのいずれの直訳にもよらないで、随...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             document\n",
       "0   三国志は、いうまでもなく、今から約千八百年前の古典であるが、三国志の中に活躍している登場人物...\n",
       "1   三国志には、詩がある。\\r\\n　単に尨大な治乱興亡を記述した戦記軍談の類でない所に、東洋人の...\n",
       "..                                                ...\n",
       "4   現在の地名と、原本の誌す地名とは、当然時代による異いがあるので、分っている地方は下に註を加え...\n",
       "5   原本には「通俗三国志」「三国志演義」その他数種あるが、私はそのいずれの直訳にもよらないで、随...\n",
       "\n",
       "[6 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = 'https://www.aozora.gr.jp/cards/001562/files/52409_ruby_51058.zip'\n",
    "file_name = url.split('/')[-1]\n",
    "out_path = Path(get_file(file_name, url, extract=True))\n",
    "with open(out_path.parent / '01jo.txt', 'rb') as f:\n",
    "    text = f.read().decode('shift_jis')\n",
    "    text = text.split('底本：')[0]\n",
    "    text = re.split(r'\\-{55,}', text)[2]\n",
    "    text = re.sub(r'｜(.+?)《.+?》', r'\\1', text)\n",
    "    text = re.sub(r'《.+?》', '', text)\n",
    "    text = re.sub(r'［＃.+?］', '', text)\n",
    "document = [paragraph.strip() for paragraph in text.split('×')]\n",
    "sangokushi = pd.DataFrame(dict(document=document))\n",
    "print('sangokushi')\n",
    "display(sangokushi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy`+`GiNZA`+`gensim`の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1800</th>\n",
       "      <th>―</th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>かしら</th>\n",
       "      <th>きっと</th>\n",
       "      <th>...</th>\n",
       "      <th>通俗</th>\n",
       "      <th>違う</th>\n",
       "      <th>選る</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1800  ―   、  。  かしら  きっと  ...  通俗  違う  選る  長所  随時  難渋\n",
       "0      1  2  14  4    1    1  ...   0   0   0   0   0   0\n",
       "1      0  0  16  7    0    0  ...   0   0   0   0   0   0\n",
       "..   ... ..  .. ..  ...  ...  ...  ..  ..  ..  ..  ..  ..\n",
       "4      0  0   6  4    0    0  ...   0   0   0   0   0   0\n",
       "5      0  0  14  3    0    0  ...   1   1   1   1   1   1\n",
       "\n",
       "[6 rows x 304 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = spacy.load('ja_ginza')\n",
    "docs = []\n",
    "for document in sangokushi['document']:\n",
    "    doc = tokenizer(document)\n",
    "    words = []\n",
    "    for sentence in doc.sents:\n",
    "        words += [token.lemma_ for token in sentence if not token.is_stop]\n",
    "    docs.append(words)\n",
    "sangokushi_bow = doc2matrix(docs)\n",
    "sangokushi_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1800</th>\n",
       "      <th>―</th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>かしら</th>\n",
       "      <th>きっと</th>\n",
       "      <th>...</th>\n",
       "      <th>要人</th>\n",
       "      <th>見る</th>\n",
       "      <th>親しむ</th>\n",
       "      <th>誰</th>\n",
       "      <th>過言</th>\n",
       "      <th>雑多</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1800  ―   、  。  かしら  きっと  ...  要人  見る  親しむ  誰  過言  雑多\n",
       "0     1  2  14  4    1    1  ...   1   1    1  1   1   1\n",
       "\n",
       "[1 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三国志は、いうまでもなく、今から約千八百年前の古典であるが、三国志の中に活躍している登場人物は、現在でも中国大陸の至る所にそのまま居るような気がする。――中国大陸へ行って、そこの雑多な庶民や要人などに接し、特に親しんでみると、三国志の中に出て来る人物の誰かしらときっと似ている。或いは、共通したものを感じる場合がしばしばある。\n",
      "　だから、現代の中国大陸には、三国志時代の治乱興亡がそのままあるし、作中の人物も、文化や姿こそ変っているが、なお、今日にも生きているといっても過言でない。\n"
     ]
    }
   ],
   "source": [
    "check_first_sample(sangokushi_bow, sangokushi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`macab`+`gensim`の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>通俗</th>\n",
       "      <th>酌む</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "      <th>頃</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 370 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ――  　   、  。  ある  いう  ...  通俗  酌む  長所  随時  難渋  頃\n",
       "0    1  1  14  4   3   2  ...   0   0   0   0   0  0\n",
       "1    0  4  16  7   4   1  ...   0   0   0   0   0  0\n",
       "..  .. ..  .. ..  ..  ..  ...  ..  ..  ..  ..  .. ..\n",
       "4    0  0   6  4   3   0  ...   0   0   0   0   0  0\n",
       "5    0  0  14  3   3   1  ...   1   1   1   1   1  1\n",
       "\n",
       "[6 rows x 370 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chasen = MeCab.Tagger('-Ochasen')\n",
    "docs = []\n",
    "for document in sangokushi['document']:\n",
    "    words = []\n",
    "    for sentence in document.split('\\r\\n'):\n",
    "        words += [\n",
    "            token.split('\\t')[2]\n",
    "            for token in chasen.parse(sentence).split('\\n')\n",
    "            if token not in ['EOS', '']\n",
    "        ]\n",
    "    docs.append(words)\n",
    "sangokushi_bow = doc2matrix(docs)\n",
    "sangokushi_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>行く</th>\n",
       "      <th>要人</th>\n",
       "      <th>親しむ</th>\n",
       "      <th>誰</th>\n",
       "      <th>過言</th>\n",
       "      <th>雑多</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ――  　   、  。  ある  いう  ...  行く  要人  親しむ  誰  過言  雑多\n",
       "0   1  1  14  4   3   2  ...   1   1    1  1   1   1\n",
       "\n",
       "[1 rows x 84 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三国志は、いうまでもなく、今から約千八百年前の古典であるが、三国志の中に活躍している登場人物は、現在でも中国大陸の至る所にそのまま居るような気がする。――中国大陸へ行って、そこの雑多な庶民や要人などに接し、特に親しんでみると、三国志の中に出て来る人物の誰かしらときっと似ている。或いは、共通したものを感じる場合がしばしばある。\n",
      "　だから、現代の中国大陸には、三国志時代の治乱興亡がそのままあるし、作中の人物も、文化や姿こそ変っているが、なお、今日にも生きているといっても過言でない。\n"
     ]
    }
   ],
   "source": [
    "check_first_sample(sangokushi_bow, sangokushi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`JUMAN++`+`gensim`の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>読者</th>\n",
       "      <th>通俗</th>\n",
       "      <th>酌む</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ――  　   、  。  ある  いう  ...  読者  通俗  酌む  長所  随時  難渋\n",
       "0    1  1  14  4   2   2  ...   0   0   0   0   0   0\n",
       "1    0  4  16  7   3   1  ...   0   0   0   0   0   0\n",
       "..  .. ..  .. ..  ..  ..  ...  ..  ..  ..  ..  ..  ..\n",
       "4    0  0   6  4   2   0  ...   0   0   0   0   0   0\n",
       "5    0  0  14  3   1   1  ...   1   1   1   1   1   1\n",
       "\n",
       "[6 rows x 367 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "juman = Juman()\n",
    "docs = []\n",
    "for document in sangokushi['document']:\n",
    "    words = []\n",
    "    for sentence in document.split('\\r\\n'):\n",
    "        words += [\n",
    "            token.genkei for token in juman.analysis(sentence).mrph_list()\n",
    "        ]\n",
    "    docs.append(words)\n",
    "sangokushi_bow = doc2matrix(docs)\n",
    "sangokushi_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>行く</th>\n",
       "      <th>要人</th>\n",
       "      <th>親しむ</th>\n",
       "      <th>誰</th>\n",
       "      <th>過言</th>\n",
       "      <th>雑多だ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ――  　   、  。  ある  いう  ...  行く  要人  親しむ  誰  過言  雑多だ\n",
       "0   1  1  14  4   2   2  ...   1   1    1  1   1    1\n",
       "\n",
       "[1 rows x 81 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三国志は、いうまでもなく、今から約千八百年前の古典であるが、三国志の中に活躍している登場人物は、現在でも中国大陸の至る所にそのまま居るような気がする。――中国大陸へ行って、そこの雑多な庶民や要人などに接し、特に親しんでみると、三国志の中に出て来る人物の誰かしらときっと似ている。或いは、共通したものを感じる場合がしばしばある。\n",
      "　だから、現代の中国大陸には、三国志時代の治乱興亡がそのままあるし、作中の人物も、文化や姿こそ変っているが、なお、今日にも生きているといっても過言でない。\n"
     ]
    }
   ],
   "source": [
    "check_first_sample(sangokushi_bow, sangokushi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf\n",
    "---\n",
    "多くの文書に出現する単語の数値は低く、少数の文書にしか出現しない単語の数値は高くなるようにし、文書の特徴を強調する指標。  \n",
    "tf (term frequency) は、1つの文書 (上のデータフレームでは1サンプル=1行) における各単語の出現頻度を、 idf (inverse document frequency) は全文書において各単語が出現する文書の割合の逆数 (の対数) を表す。\n",
    "\n",
    "文書数を $n$ 、全文書中に出現する単語の種類を $m$ 、文書 $j$ に単語 $i$ の出現する回数を $c_{i,j}$ 、文書 $j$ に単語 $i$ が出現するかどうかを表す変数 (出現する場合は1、出現しない場合は0をとる2値変数) を $d_{i,j}$ とすると、 tf-idf は以下の $tf_{i,j}$ ・ $idf_{i}$ の積で表される。\n",
    "\n",
    "$\n",
    "\\displaystyle \\begin{cases}\n",
    "    tf_{i,j} & ={\\displaystyle \\frac\n",
    "        {c_{i,j}}\n",
    "        {{\\displaystyle \\sum ^{m}_{k=1} c_{k,j}}}\n",
    "    }\\\\\n",
    "    idf_{i} & ={\\displaystyle log\\frac\n",
    "        {n}\n",
    "        {{\\displaystyle \\sum ^{n}_{k=1} d_{i,k}}}\n",
    "    }\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`sangokushi`データセットのBoWを表す`sangokushi_bow`を元に、出現回数の値をtfに置き換えた`sangokushi_tf`を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sangokushi_bow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>読者</th>\n",
       "      <th>通俗</th>\n",
       "      <th>酌む</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ――  　   、  。  ある  いう  ...  読者  通俗  酌む  長所  随時  難渋\n",
       "0    1  1  14  4   2   2  ...   0   0   0   0   0   0\n",
       "1    0  4  16  7   3   1  ...   0   0   0   0   0   0\n",
       "..  .. ..  .. ..  ..  ..  ...  ..  ..  ..  ..  ..  ..\n",
       "4    0  0   6  4   2   0  ...   0   0   0   0   0   0\n",
       "5    0  0  14  3   1   1  ...   1   1   1   1   1   1\n",
       "\n",
       "[6 rows x 367 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('sangokushi_bow')\n",
    "display(sangokushi_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>読者</th>\n",
       "      <th>通俗</th>\n",
       "      <th>酌む</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.094595</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089172</td>\n",
       "      <td>0.019108</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ――         　         、         。        ある        いう  ...        読者  \\\n",
       "0   0.006757  0.006757  0.094595  0.027027  0.013514  0.013514  ...  0.000000   \n",
       "1   0.000000  0.020833  0.083333  0.036458  0.015625  0.005208  ...  0.000000   \n",
       "..       ...       ...       ...       ...       ...       ...  ...       ...   \n",
       "4   0.000000  0.000000  0.071429  0.047619  0.023810  0.000000  ...  0.000000   \n",
       "5   0.000000  0.000000  0.089172  0.019108  0.006369  0.006369  ...  0.006369   \n",
       "\n",
       "          通俗        酌む        長所        随時        難渋  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.006369  0.006369  0.006369  0.006369  0.006369  \n",
       "\n",
       "[6 rows x 367 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sangokushi_tf = sangokushi_bow.div(sangokushi_bow.sum(axis='columns'),\n",
    "                                   axis='rows')\n",
    "sangokushi_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`sangokushi`データセットのBoWを表す`sangokushi_bow`を元に、各単語のidfを表す`sangokushi_idf`を作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "――    1.791759\n",
       "　     1.098612\n",
       "        ...   \n",
       "随時    1.791759\n",
       "難渋    1.791759\n",
       "Length: 367, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sangokushi_binary = sangokushi_bow.astype(bool)\n",
    "sangokushi_df = sangokushi_binary.sum(axis='rows') / sangokushi_bow.index.size\n",
    "sangokushi_idf = -np.log(sangokushi_df)\n",
    "sangokushi_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習問題\n",
    "\n",
    "---\n",
    "`sangokushi_tf`と`sangokushi_idf`から、`sangokushi_bow`の各値をtf-idfに置き換えた`sangokushi_tfidf`を作成し、各文書のBoW上位5単語とtf-idf上位5単語を比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "解答例\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words\n",
      "[['、' 'の' 'に' 'が']\n",
      " ['、' 'に' 'を' 'の']\n",
      " ['の' '、' 'に' 'と']\n",
      " ['、' 'の' 'と' 'も']\n",
      " ['、' 'の' '。' 'は']\n",
      " ['、' 'に' 'の' 'は']]\n",
      "tf-idf\n",
      "[['大陸' '中国' 'そのまま' '中']\n",
      " ['\\u3000' '詩' '搏' 'しまう']\n",
      " ['代' 'さ' '中国' '年']\n",
      " ['民俗' 'られる' '彩る' '生']\n",
      " ['地名' '文字' '分る' '特有だ']\n",
      " ['寝る' '演義' '「' '」']]\n"
     ]
    }
   ],
   "source": [
    "sangokushi_tfidf = sangokushi_tf.mul(sangokushi_idf, axis='columns')\n",
    "columns = sangokushi_bow.columns.values\n",
    "print('Bag of Words')\n",
    "print(columns[np.argsort(sangokushi_bow)][:, :-5:-1])\n",
    "print('tf-idf')\n",
    "print(columns[np.argsort(sangokushi_tfidf)][:, :-5:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pythonでのtf-idf作成方法\n",
    "---\n",
    "`sklearn.feature_extraction.text.TfidfVectorizer`または`sklearn.feature_extraction.text.TfidfTransformer`を使用する。  \n",
    "デフォルト設定は計算の安定その他の理由から、上記 tf-idf の説明とは少し異なる値を返すようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class TfidfVectorizer(CountVectorizer)\n",
      " |  TfidfVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |  \n",
      " |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      " |  \n",
      " |  Equivalent to :class:`CountVectorizer` followed by\n",
      " |  :class:`TfidfTransformer`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : str, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode'}, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      " |      list is returned. 'english' is currently the only supported string\n",
      " |      value.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : str\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      " |      unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      " |      only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  max_df : float or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      " |      documents, integer absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      " |      of documents, integer absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents.\n",
      " |  \n",
      " |  binary : bool, default=False\n",
      " |      If True, all non-zero term counts are set to 1. This does not mean\n",
      " |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      " |      is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
      " |  \n",
      " |  dtype : dtype, default=float64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  norm : {'l1', 'l2'}, default='l2'\n",
      " |      Each output row will have unit norm, either:\n",
      " |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      " |      similarity between two vectors is their dot product when l2 norm has\n",
      " |      been applied.\n",
      " |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      " |      See :func:`preprocessing.normalize`.\n",
      " |  \n",
      " |  use_idf : bool, default=True\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : bool, default=True\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : bool, default=False\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: bool\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  idf_ : array of shape (n_features,)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if ``use_idf`` is True.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      " |  \n",
      " |  TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      " |      matrix of counts.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = TfidfVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.shape)\n",
      " |  (4, 9)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfVectorizer\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf from training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      y : None\n",
      " |          This parameter is not needed to compute tfidf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted vectorizer.\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf, return document-term matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of (n_samples, n_features)\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  transform(self, raw_documents, copy='deprecated')\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      " |      fit_transform).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      copy : bool, default=True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |          .. deprecated:: 0.22\n",
      " |             The `copy` parameter is unused and was deprecated in version\n",
      " |             0.22 and will be removed in 0.24. This parameter will be\n",
      " |             ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of (n_samples, n_features)\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  norm\n",
      " |  \n",
      " |  smooth_idf\n",
      " |  \n",
      " |  sublinear_tf\n",
      " |  \n",
      " |  use_idf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CountVectorizer:\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zionists</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18844</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18846 rows × 8061 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  aaron   ab  abandon  abandoned  ...  zionist  zionists  zip  \\\n",
       "0      0.0  0.0    0.0  0.0      0.0        0.0  ...      0.0       0.0  0.0   \n",
       "1      0.0  0.0    0.0  0.0      0.0        0.0  ...      0.0       0.0  0.0   \n",
       "...    ...  ...    ...  ...      ...        ...  ...      ...       ...  ...   \n",
       "18844  0.0  0.0    0.0  0.0      0.0        0.0  ...      0.0       0.0  0.0   \n",
       "18845  0.0  0.0    0.0  0.0      0.0        0.0  ...      0.0       0.0  0.0   \n",
       "\n",
       "       zone  zoom   zx  \n",
       "0       0.0   0.0  0.0  \n",
       "1       0.0   0.0  0.0  \n",
       "...     ...   ...  ...  \n",
       "18844   0.0   0.0  0.0  \n",
       "18845   0.0   0.0  0.0  \n",
       "\n",
       "[18846 rows x 8061 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             token_pattern='(?u)\\\\b[a-z][a-z]+\\\\b',\n",
    "                             min_df=20)\n",
    "vec = vectorizer.fit_transform(news['document'])\n",
    "news_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vec, columns=vectorizer.get_feature_names())\n",
    "news_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfTransformer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class TfidfTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |  \n",
      " |  Transform a count matrix to a normalized tf or tf-idf representation\n",
      " |  \n",
      " |  Tf means term-frequency while tf-idf means term-frequency times inverse\n",
      " |  document-frequency. This is a common term weighting scheme in information\n",
      " |  retrieval, that has also found good use in document classification.\n",
      " |  \n",
      " |  The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
      " |  token in a given document is to scale down the impact of tokens that occur\n",
      " |  very frequently in a given corpus and that are hence empirically less\n",
      " |  informative than features that occur in a small fraction of the training\n",
      " |  corpus.\n",
      " |  \n",
      " |  The formula that is used to compute the tf-idf for a term t of a document d\n",
      " |  in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
      " |  computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
      " |  n is the total number of documents in the document set and df(t) is the\n",
      " |  document frequency of t; the document frequency is the number of documents\n",
      " |  in the document set that contain the term t. The effect of adding \"1\" to\n",
      " |  the idf in the equation above is that terms with zero idf, i.e., terms\n",
      " |  that occur in all documents in a training set, will not be entirely\n",
      " |  ignored.\n",
      " |  (Note that the idf formula above differs from the standard textbook\n",
      " |  notation that defines the idf as\n",
      " |  idf(t) = log [ n / (df(t) + 1) ]).\n",
      " |  \n",
      " |  If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
      " |  numerator and denominator of the idf as if an extra document was seen\n",
      " |  containing every term in the collection exactly once, which prevents\n",
      " |  zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n",
      " |  \n",
      " |  Furthermore, the formulas used to compute tf and idf depend\n",
      " |  on parameter settings that correspond to the SMART notation used in IR\n",
      " |  as follows:\n",
      " |  \n",
      " |  Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
      " |  ``sublinear_tf=True``.\n",
      " |  Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
      " |  Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
      " |  when ``norm=None``.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  norm : {'l1', 'l2'}, default='l2'\n",
      " |      Each output row will have unit norm, either:\n",
      " |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      " |      similarity between two vectors is their dot product when l2 norm has\n",
      " |      been applied.\n",
      " |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      " |      See :func:`preprocessing.normalize`\n",
      " |  \n",
      " |  use_idf : bool, default=True\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : bool, default=True\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : bool, default=False\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  idf_ : array of shape (n_features)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if  ``use_idf`` is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import TfidfTransformer\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> from sklearn.pipeline import Pipeline\n",
      " |  >>> import numpy as np\n",
      " |  >>> corpus = ['this is the first document',\n",
      " |  ...           'this document is the second document',\n",
      " |  ...           'and this is the third one',\n",
      " |  ...           'is this the first document']\n",
      " |  >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
      " |  ...               'and', 'one']\n",
      " |  >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
      " |  ...                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
      " |  >>> pipe['count'].transform(corpus).toarray()\n",
      " |  array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
      " |         [1, 2, 0, 1, 1, 1, 0, 0],\n",
      " |         [1, 0, 0, 1, 0, 1, 1, 1],\n",
      " |         [1, 1, 1, 1, 0, 1, 0, 0]])\n",
      " |  >>> pipe['tfid'].idf_\n",
      " |  array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
      " |         1.        , 1.91629073, 1.91629073])\n",
      " |  >>> pipe.transform(corpus).shape\n",
      " |  (4, 8)\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
      " |                 Information Retrieval. Addison Wesley, pp. 68-74.\n",
      " |  \n",
      " |  .. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).\n",
      " |                 Introduction to Information Retrieval. Cambridge University\n",
      " |                 Press, pp. 118-120.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Learn the idf vector (global term weights).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : sparse matrix of shape n_samples, n_features)\n",
      " |          A matrix of term/token counts.\n",
      " |  \n",
      " |  transform(self, X, copy=True)\n",
      " |      Transform a count matrix to a tf or tf-idf representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : sparse matrix of (n_samples, n_features)\n",
      " |          a matrix of term/token counts\n",
      " |      \n",
      " |      copy : bool, default=True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      vectors : sparse matrix of shape (n_samples, n_features)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>――</th>\n",
       "      <th></th>\n",
       "      <th>、</th>\n",
       "      <th>。</th>\n",
       "      <th>ある</th>\n",
       "      <th>いう</th>\n",
       "      <th>...</th>\n",
       "      <th>読者</th>\n",
       "      <th>通俗</th>\n",
       "      <th>酌む</th>\n",
       "      <th>長所</th>\n",
       "      <th>随時</th>\n",
       "      <th>難渋</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.07497</td>\n",
       "      <td>0.061476</td>\n",
       "      <td>0.465906</td>\n",
       "      <td>0.133116</td>\n",
       "      <td>0.066558</td>\n",
       "      <td>0.088953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.209835</td>\n",
       "      <td>0.454361</td>\n",
       "      <td>0.198783</td>\n",
       "      <td>0.085193</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314257</td>\n",
       "      <td>0.209505</td>\n",
       "      <td>0.104752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442520</td>\n",
       "      <td>0.094826</td>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.042244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.071207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 367 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ――         　         、         。        ある        いう  ...        読者  \\\n",
       "0   0.07497  0.061476  0.465906  0.133116  0.066558  0.088953  ...  0.000000   \n",
       "1   0.00000  0.209835  0.454361  0.198783  0.085193  0.037953  ...  0.000000   \n",
       "..      ...       ...       ...       ...       ...       ...  ...       ...   \n",
       "4   0.00000  0.000000  0.314257  0.209505  0.104752  0.000000  ...  0.000000   \n",
       "5   0.00000  0.000000  0.442520  0.094826  0.031609  0.042244  ...  0.071207   \n",
       "\n",
       "          通俗        酌む        長所        随時        難渋  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.071207  0.071207  0.071207  0.071207  0.071207  \n",
       "\n",
       "[6 rows x 367 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "vec = transformer.fit_transform(sangokushi_bow)\n",
    "sangokushi_tfidf = pd.DataFrame.sparse.from_spmatrix(\n",
    "    vec, columns=sangokushi_bow.columns)\n",
    "sangokushi_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推薦図書\n",
    "---\n",
    "- [見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑](https://www.amazon.co.jp/%E8%A6%8B%E3%81%A6%E8%A9%A6%E3%81%97%E3%81%A6%E3%82%8F%E3%81%8B%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E4%BB%95%E7%B5%84%E3%81%BF-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E5%9B%B3%E9%91%91-%E7%A7%8B%E5%BA%AD-%E4%BC%B8%E4%B9%9F/dp/4798155659/)\n",
    "- [Python 機械学習プログラミング 達人データサイエンティストによる理論と実践](https://www.amazon.co.jp/Python-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0-%E9%81%94%E4%BA%BA%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%83%86%E3%82%A3%E3%82%B9%E3%83%88%E3%81%AB%E3%82%88%E3%82%8B%E7%90%86%E8%AB%96%E3%81%A8%E5%AE%9F%E8%B7%B5-impress-gear/dp/4295003379/)\n",
    "- [Kaggleで勝つデータ分析の技術](https://www.amazon.co.jp/Kaggle%E3%81%A7%E5%8B%9D%E3%81%A4%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E3%81%AE%E6%8A%80%E8%A1%93-%E9%96%80%E8%84%87-%E5%A4%A7%E8%BC%94/dp/4297108437/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
